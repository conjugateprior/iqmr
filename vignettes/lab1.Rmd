---
title: "Lab 1"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_width: 7
    fig_height: 7 
vignette: >
  %\VignetteIndexEntry{Lab 1}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction to `quanteda`

### Building a corpus

Let's start by reading in the data from the abortion debate analyzed by Bara et al.  I've concatenated each speaker's contributions into a single file.
(This is certainly not the only way to think about analyzing this data, but it's what Bara et al. did.)

First load the package
```{r}
library(quanteda) # for general text analysis
library(quanteda.dictionaries)
library(readtext) # for reading in all kinds of files
library(dplyr)

library(iqmr)
```
then read in some text files and make a `corpus` from them
```{r}
data("corpus_bara_speaker")
```
`corpus` are somewhat compex objects, designed for holding very large amounts 
of text.  Here are the basics:

To get a basic summary of the texts and document variables
```{r}
summary(corpus_bara_speaker)
```
Those word and sentence counts come from the `ntype`, (vocabulary size)
`ntoken` (word count), and `nsentence` (sentence count)
functions.

To extract the 12th speaker's contribution
```{r}
corpus_bara_speaker[12]
```
Notice that this is a named character vector.  (`corpus_bara_speaker[13:14]` would also work).

To get all the texts
```{r, eval = FALSE}
texts(corpus_bara_speaker)
```
but maybe don't, or assign it to something, else it'll splurge into your console for a while.

To pull out one of the docvars, you can treat the corpus as if it has document
variables as columns.  Here are just the speaker voting decisions 
```{r}
corpus_bara_speaker[,'vote']
```
(`corpus_bara_speaker[['vote']]` also works, if you prefer to get a one column data frame).

To get the lot of them as a data frame
```{r}
dvars <- docvars(corpus_bara_speaker)
```

To get just *some* of the documents there is a subset command that looks
to the document variables to say which documents should be kept.  Here are all
the speakers that voted 'no' at the end of the debate and said more than 100
words during it
```{r}
no_corp <- corpus_subset(corpus_bara_speaker, vote == "no" & ntoken(corpus_bara_speaker) > 100)
```
(Goodbye Mr Mahon)

Finally, it's sometimes convenient to be able to switch between thinking 
sets of documents to sets of paragraphs, or even
sentences.
```{r}
para_corp <- corpus_reshape(corpus_bara_speaker, to = "paragraphs") # or "sentences"
head(summary(para_corp)) # Just the top few lines
```
Happily we can always reverse this process by changing `to` back to "documents".

<!-- ```{r} -->
<!-- txts <- readtext("data/abortion-debate-by-speaker/*.txt") -->
<!-- corp <- corpus(txts) -->
<!-- ``` -->
<!-- Corpora get big quickly, so most functions in the package will not show you all the contents of any object.  Call `summary` to get a view of the new corpus object. -->

<!-- It's helpful to add some metadata to the documents, so we can subset them.  Here we'll record the vote of each speaker. -->
<!-- ```{r} -->
<!-- vote <- c("abs", "abs", "abs", "no", "no", "no", "no", -->
<!--           "no", "yes", "yes", "yes", "yes", "yes", "yes", -->
<!--           "yes", "yes", "yes", "yes", "yes", "yes", "yes", -->
<!--           "yes", "yes", "yes") -->
<!-- docvars(corp, "vote") <- vote -->
<!-- summary(corp) -->
<!-- ``` -->
<!-- where `docvars` adds document specific metadata, here the speaker's vote. -->



<!-- Conversely, if we want to the texts in this corpus object without the docvars we use `texts`. -->
<!-- Maybe don't do call this if your corpus is even moderately large as it will take while to  -->
<!-- print to the screen.  To just get the contributions of the (magnificently named) Mr Norman St John-Stevas, we can index into it.  He's the 5th `document'. -->
<!-- ```{r, eval=FALSE} -->
<!-- texts(corp)[5] -->
<!-- ``` -->
<!-- To see just a few speakers, e.g. the ones that voted against, we can use `subset` -->
<!-- ```{r} -->
<!-- nocorp <- corpus_subset(corp, vote == "no") -->
<!-- summary(nocorp) -->
<!-- ``` -->
<!-- or we could use it to remove very short speeches -->
<!-- ```{r eval=FALSE} -->
<!-- longcorp <- corpus_subset(corp, words_spoken > 50) -->
<!-- ``` -->
<!-- Actually we could use the more flexible `corpus_trim`, like this: -->
<!-- ```{r} -->
<!-- longcorp <- corpus_trim(corp, what = "documents", min_ntoken = 50) -->
<!-- ``` -->

<!-- dd <- dfm(corp,remove_punct = TRUE, remove = stopwords("english")) -->
<!-- textstat_frequency(dd),  -->
<!--     n = 20, groups = "vote") -->

<!-- plot(hclust(textstat_dist(dd, method = "hamming"))) # nope -->

<!-- # aggregate rows -->
<!-- dfm(corp, remove_punct = TRUE, remove = stopwords("english"), groups = "vote") -->
<!-- # delete aggregate columns -->
<!-- # dict <- ... -->
<!-- dfm(corp, remove_punct = TRUE, remove = stopwords("english"), dictionary = dict) -->

Let's explore a little more by looking for the key terms in play.  
One way to do this is to look for collocations.  The collocation finder functions operates on the tokens of the corpus, so we extract them first
```{r}
toks <- tokens(corpus_bara_speaker)
colls <- textstat_collocations(toks)
head(colls, 20)
```
This is disappointing unsubstantive, but if we work a bit harder we can get better
results.  First we'll remove those stopwords (leaving gaps where they were).
```{r}
toks2 <- tokens_remove(toks, stopwords(), padding = TRUE)
```
Now rerun the function, maintaining the capitalization
```{r}
coll2 <- textstat_collocations(toks2, tolower = FALSE, size = 2)
head(coll2, 20)
```
We can also ask for three word collocations
```{r}
coll3 <- textstat_collocations(toks2, tolower = FALSE, size = 3)
head(coll3, 30)
```
If we're *really* serious about collocation hunting, it's probably best to use a dedicated package.

Let's get a bit more confirmatory in our text analysis

### Keywords in context

Since this is an abortion debate, let's see the honorable folk talk about 
mothers and babies. We'll use the `keyword in context' function
```{r}
kw_mother <- kwic(corpus_bara_speaker, "mother*", window = 10)  # 10 words either side of every instance
head(kw_mother)
```
KWICs can get quite large, but if you want to see it all
```{r, eval = FALSE}
View(kw_mother)
```
will open a browser with the whole thing.

There is much less talk of babies than of mothers.
In this debate, the other major actors are doctors and their 
professional association, which you can investigate the same way.

The output of `kwic` is simply a data frame, so one thing that's often useful 
is to treat the left and right sides of the kwic as a document (about babies), 
e.g. like this:
```{r}
babes <- kwic(corpus_bara_speaker, "babi*", window = 10)
txt <- paste(babes$pre, babes$post, collapse=" ") # make one big string
```
This constructed document contains, by definition, all the ways the term was 
used in the corpus, so you can then examine what sorts of words tend to be used 
around it, e.g. by using a content analysis dictionary.

As far as I know this was pioneered by the Yoshikoder software, 
and bears some obvious similarities to the word-embedding analyses that are 
currently fashionable.

Now let's get even more confirmatory, and apply the mapping between words and 
topics described by Bara et al.'s dictionary to this corpus, in order to 
replicate their first analysis.

Our first stem will be to create a document feature matrix (`dfm`), after 
which dictionary application is straightforward.  But since lots of models require 
a dfm, we'll linger a little on the steps of the process.

### Constructing a document feature matrix

quanteda makes a basic dfm quite straightforward
```{r}
corpdfm <- dfm(corpus_bara_speaker) # lowercases by default, but not much more
dim(corpdfm)
featnames(corpdfm)[1:40] # really just colnames
```
But let's remove some things that aren't (currently) of interest to us
```{r}
corpdfm <- dfm(corpus_bara_speaker, 
               remove = stopwords(), 
               remove_punct = TRUE,
               remove_numbers = TRUE)
dim(corpdfm) # a bit smaller
featnames(corpdfm)[1:40]
```
We *could* also stem
```{r}
stemdfm <- dfm(corpus_bara_speaker, remove=stopwords(), remove_punct=TRUE,
               remove_numbers=TRUE, stem=TRUE)
dim(stemdfm) # about 1000 fewer 'word's
featnames(stemdfm)[1:40]
```
but our dictionary entries aren't stemmed, so let's save `stemdfm` for later.

For modeling, we'll often want to remove the low frequency and idiosyncratic words
```{r}
smallcorpdfm <- dfm_trim(corpdfm, min_termfreq = 5, min_docfreq=5)
dim(smallcorpdfm)
```
where `min_count` removes any word that occurs less than 5 times and `min_docfreq` removes any words that occurs any number of times but  in fewer than 5 documents.  That makes things a fair bit smaller.  But again, we don't want to miss dictionary entries.

There's also a `wordcloud` function for viewing the the document feature matrix, 
but we won't use it because wordclouds are silly.

## Answering questions with text

In the debate the Speaker, Mr Horace King, said he would try to give equal time to both sides of the debate.  Did it happen this way?

It's hard to know whether the debate was persuasive since we do not know the speakers prior beliefs (though we could find out from their previous debates) so let us assume that there was no substantial persuasion and we'll assume that no speaker spoke particularly slowly.  
These imply that we can proxy speaking time with number of words said, 
and side with final vote.
```{r}
df <- docvars(corpus_bara_speaker)
df$word_count <- ntoken(corpus_bara_speaker)
df %>%
  group_by(vote) %>%
  summarise(floortime = sum(word_count),
            speakers = length(vote))
```
Although three quarters of the speakers voted "yes", it seems that floor time was 
about two to one "yes" to "no" voters.

### Applying a content analysis dictionary

Let's turn to the content analysis dictionary that Bara used.
A content analysis dictionary in `quanteda` terms can be made out of a list of vectors of words, like this:
```{r}
dictionary(list(medics = c("doctor", "medical", "hospital"),
                mothers = c("mother", "parents")))
```
or imported in the format of some other content analysis program.
(Quanteda can deal with dictionaries from Wordstat, Yoshikoder, and LIWC). 
We'll read the dictionary in Yoshikoder format:
```{r}
baradic <- dictionary(file = "data/bara-dict-corrected.ykd")
```

<!-- baradic_stems <- rapply(baradic, char_wordstem, how = "replace") -->

### Replicating a little bit of Bara

With dictionary in hand we can now go *category* counting rather than word counting
```{r}
baradfm <- dfm(corpus_bara_speaker, dictionary = baradic)
```
Since this output is not absolutely massive
```{r}
dim(baradfm)
```
let's force it into a regular R matrix to take a look at the whole thing without being swamped in elements
```{r}
dictout <- as.matrix(baradfm)
dictout
```
And recreate some of Bara et al.'s Table 3, repeated below, as a bar plot.

```{r, results="asis"}
tab <- data.frame(Mean = c(13.59, 7.82, 21.71, 4.61, 32.17, 20.09),
                  SD = c(2.98, 3.36, 4.73, 2.51, 6.94, 4.86),
                  row.names = c("advocacy", "legal", "medical", "moral", "procedural", "social" ),
                  stringsAsFactors = FALSE)
knitr::kable(t(tab))
```

```{r}
prop_emph <- (dictout / rowSums(dictout))
tab <- data.frame(Mean = 100 * apply(prop_emph, 2, mean),
                  SD = 100 * apply(prop_emph, 2, sd),
                  row.names = c("advocacy", "legal", "medical", "moral", "procedural", "social" ),
                  stringsAsFactors = FALSE)
```

```{r, results = "asis"}
knitr::kable(t(tab))
```

Finally, let's revisit the floortime question but this time counting only 
vocabulary that Bara et al. thought was substantively relevant.
```{r}
df$relevant_word_count <- rowSums(dictout)
df %>%
  group_by(vote) %>%
  summarise(floortime = sum(relevant_word_count),
            speakers = length(vote))
```

Not much change.

<!-- %' -->
<!-- %' \subsection*{Abortion Politics in the US} -->
<!-- %' -->
<!-- %' Now we will compare this legislative debate with a similar one from the US Senate. This is the debate on the conference report for the Partial-Birth Abortion Ban Act of 2003. -->
<!-- %' -->
<!-- %' Import the US speech documents into Yoshikoder, re-do the dictionary analysis -->
<!-- %' <<>>= -->
<!-- %' us<-read_ykd_report("usabortion-utf8.csv",drop.total=TRUE) -->
<!-- %' @ -->
<!-- %' -->
<!-- %' Calculate per speaker proportions -->
<!-- %' <<>>= -->
<!-- %' usprop<-us/rowSums(us) -->
<!-- %' @ -->
<!-- %' -->
<!-- %' Select Democrats and Republicans -->
<!-- %' <<>>= -->
<!-- %' dems<-usprop[grep("DEM",rownames(usprop)),] -->
<!-- %' reps<-usprop[grep("REP",rownames(usprop)),] -->
<!-- %' @ -->
<!-- %' -->
<!-- %' Calculate average proportions for Dems and Reps -->
<!-- %' <<>>= -->
<!-- %' demavg<-apply(dems,2,mean) -->
<!-- %' repavg<-apply(reps,2,mean) -->
<!-- %' @ -->
<!-- %' -->
<!-- %' Create one object -->
<!-- %' <<>>= -->
<!-- %' usres<-rbind(demavg,repavg) -->
<!-- %' rownames(usres)<-c("Dem","Rep") -->
<!-- %' @ -->
<!-- %' and plot it -->
<!-- %' <<>>= -->
<!-- %' barplot(usres,beside=T,legend=T,ylim=c(0,0.4)) -->
<!-- %' @ -->
<!-- % -->
<!-- % \begin{itemize} -->
<!-- % \item Do you find a similar or different distribution of keyword occurrences in the debate categories as defined by Bara et al.? -->
<!-- % \item Can you find a partisan pattern in the data? To do this, compute averages of the category percentages separately for Democrats and Republicans. -->
<!-- % \item More generally, where do you see the benefits and/or limitations of applying this specific dictionary? Do you suspect that important terms are omitted? Notice that there is no ``gender politics'' category in the Bara et al. dictionary. Can you think of some terms that might reflect gender politics? Use Yoshikoder to add a gender category and appropriate patterns. Re-apply the dictionary  to the data. Do you find a partisan pattern? -->
<!-- % \end{itemize} -->


<!-- % \subsection*{Party Politics in the UK} -->
<!-- % -->
<!-- % Finally, we look at the manifesto policy dictionary constructed by Laver and Garry (2000) and the development of economic policy positions, in particular. First, load the dictionary and the British party manifestos for the elections 1992 to 2010. -->
<!-- % -->
<!-- % Our goal is twofold: first, we would like to replicate the result that the Labour Party moved towards a neoliberal economic policy position in 1997 under Tony Blair, and want to examine how stable this move was in the subsequent elections. -->
<!-- % -->
<!-- % Open the dictionary file in Excel and calculate the economic policy positions for the parties as follows: -->
<!-- % \begin{equation*} -->
<!-- %   \text{ECON} = \frac{\text{ECON}_R - \text{ECON}_L}{\text{ECON}_R + \text{ECON}_L} -->
<!-- % \end{equation*} -->
<!-- % where $\text{ECON}_L$ is the total counts in category ``increase role of state in the economy'' and -->
<!-- % $ECON_R$ is the total counts in category ``reduce role of state in the economy''. -->
<!-- % \begin{itemize} -->
<!-- % \item Using Excel, plot the estimates for the parties over time (hint: you may need to re-arrange the data with parties in rows and years in columns).  Can you replicate the rightward move of Labour in 1997? Does Labour stay on the right in subsequent years? -->
<!-- % \item Following the election in 2010, none of the parties could muster a majority in parliament. Subsequently, the Conservative and Liberal Democratic party formed a coalition government, and Labour went into the opposition. Based on your estimates of economic policy positions derived from the dictionary counts, does this coalition appear sensible from the Liberal Democratic party's perspective? -->
<!-- % \end{itemize} -->

