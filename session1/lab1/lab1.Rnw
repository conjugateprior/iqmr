\documentclass[10pt,a4paper]{article}
%\usepackage[utf8]{inputenc}
\usepackage[charter]{mathdesign}
\usepackage[scaled=.95]{inconsolata}
\usepackage[margin=1.1in]{geometry}
\usepackage{color}

\usepackage{hyperref}
\usepackage{dcolumn,booktabs} %% for memisc
\usepackage{graphicx}
\usepackage{amsmath}

\definecolor{darkblue}{rgb}{0,0,.6} % not really
\definecolor{other}{rgb}{0,0,.5}
\hypersetup{colorlinks=true, linkcolor=darkblue, citecolor=darkblue, 
	filecolor=darkblue,urlcolor=other}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}


\author{IQMR 2016}
\title{Lab 1}

\date{}

\begin{document}

\maketitle

<<include=FALSE>>=
knitr::opts_chunk$set(error=FALSE, 
                      comment='',
                      size="small")
options(width=100)
@


\section{Introduction to \texttt{quanteda}}

\subsection*{Building a corpus}

Let's start by reading in the data from the abortion debate analyzed by Bara et al.  I've concatenated each speaker's contributions into a single file.
(This is certainly not the only way to think about analyzing this data, but it's what Bara et al. did.)

First load the package
<<>>=
library(quanteda)
@
then read in some text files and make a \texttt{corpus} from them
<<>>=
txts <- textfile("data/abortion-debate-by-speaker/*")
corp <- corpus(txts)
@
Corpora get big quickly, so most functions in the package will not show you all the contents of any object.  Call \texttt{summary} to get a view of the new corpus object.

It's helpful to add some metadata to the documents, so we can subset them.  Here we'll record the vote of each speaker.
<<>>=
vote <- rep("yes", 24) ## 16 voted yes
vote[1:3] <- "abs"     ## 3 abstained
vote[4:8] <- "no"      ## 5 voted no
docvars(corp, "vote") <- vote
summary(corp)
@
where \texttt{docvars} adds document specific metadata, here the speaker's vote.

If we want to get the texts \textit{out} of this corpus object we use
<<eval=FALSE>>=
texts(corp)
@
To just get the contributions of the delightfully named Mr Norman St John-Stevas, we can index into it
<<eval=FALSE>>=
texts(corp)[5]
@
To see just a few speakers, e.g. the ones that voted against, we can use \texttt{subset}
<<>>=
nocorp <- subset(corp, vote == "no")
summary(nocorp)
@

\subsection*{Exploring text corpora}

Let's check to see if there is key terminology that we should be looking out for.  One way to do this is to look for collocations. These are word combinations that occur more often than we would expect from their individual frequencies.  Here's the top 40.
<<>>=
collocations(corp, n=40)
@
This reminds us that some key terms are \texttt{royal college} (of surgeons), \texttt{illegal abortions}, \texttt{medical profession}, and the more procedural \texttt{right hon} and \texttt{second reading}.

This works better, the larger the corpus. You can tweak the results by changing the statistic used to score word pairs.  If you have a little time on your hands and a corpus larger than this one then you can also look for word triples, although finding them is a bit more computationally intensive.

Here's an example of using pointwise mutual information (\texttt{pmi}) rather than a likelihood ratio test (\texttt{lr}) to find three word collocations
<<eval=FALSE>>=
collocations(corp, size=3, n=40, method="pmi")
@
The first couple of terms remind us that this debate happened in the Sixties\ldots 

\subsection*{Keywords in context}

Since this is an abortion debate, let's see the honourable folk talk about mothers and babies. We'll use the keyword in context function
<<eval=FALSE>>=
kwic(corp, "mother*")
@
we might benefit from a bit more local context, so maybe set the window a bit wider  Here are the babies
<<eval=FALSE>>=
kwic(corp, "babi*", window=10)
@
you may need to expand your window a bit to see these properly.

Perhaps oddly, there is much less talk of babies than of mothers.  
In this debate, the other major actors are doctors and their professional association, which you can investigate the same way.

\subsection*{Constructing a document feature matrix}

One of the first things we tend to do to a set of documents as preparation from modeling is to make a document feature matrix (\texttt{dfm})
<<>>=
corpdfm <- dfm(corp)
dim(corpdfm)
@
Typically though, we'll want to trim the low frequency and idiosyncratic terms out
<<>>=
corpdfm2 <- trim(corpdfm, minCount=5, minDoc=5)
dim(corpdfm2)
@
which makes it a fair bit smaller.

There's also a \texttt{wordcloud} function for viewing the the document feature matrix, but we won't use it because wordclouds are silly.

\subsection*{Answering questions with text}

Now let's prod these documents in a more substantively focused way.  

In the debate the Speaker, Mr Horace King, said he would try to give equal time to both sides of the debate.  (You can read the original debate as \texttt{data/abortion-debate-hansard.html}).  Did it happen this way?  

It's hard to know whether the debate was persuasive since we do not know the speakers prior beliefs (though we could find out from their previous debates) so let us assume that there was no substantial persuasion.  We'll also assume that no speaker spoke particularly slowly.  These imply that we can proxy speaking time with number of words said.
<<>>=
speakingtime <- rowSums(corpdfm)
speakingtime
@
Now to break this down by final vote 
<<>>=
aggregate(speakingtime ~ docvars(corp, "vote"), FUN=sum)
@
It appears that floor time was about 30\% eventual no voters and 60\% eventual yes voters.  However, individual no voters did get on average more time each
<<>>=
aggregate(speakingtime ~ docvars(corp, "vote"), FUN=mean)
@

\subsection*{Applying a content analysis dictionary}

Let's turn to the content analysis dictionary that Bara used.
A content analysis dictionary in \texttt{quanteda} terms can be a regular R list of words.  (It can also import Wordstat and LIWC format files, if you made a dictionary in one of those packages).  Here we'll just read in the Bara dictionary to be the right kind of list.

I have a copy of the dictionary in Yoshikoder format, an XML format that can be rather easily parsed by the \texttt{rvest} web scraping package.  So I'll use that.  If you have a different format you might have to wrote this yourself (or you can ask us how).  It's not pretty, but for the record
<<>>=
library(rvest)
dic <- read_html("data/2007_abortion_dictionary.ykd")
cats <- html_nodes(dic, "cnode cnode") # top level categories
getwords <- function(x){ html_attr(html_nodes(x, "pnode"), "name") }
baradic <- lapply(cats, getwords)
names(baradic) <- html_attr(cats, "name")
@
We take this simple list and turn it into a quanteda \texttt{dictionary} object
<<>>=
bara <- dictionary(baradic)
@

\subsection*{Replicating a little bit of Bara}

With dictionary in hand we can now go \textit{category} counting rather than word counting
<<>>=
baradfm <- dfm(corp, dictionary=bara)
@
Since this output is not absolutely massive
<<>>=
dim(baradfm)
@
let's force it into a regular R matrix to take a look at the whole thing without being swamped in elements
<<>>=
dictout <- as.matrix(baradfm)
dictout
@
And recreate some of Bara et al.'s Table 3, only as a bar plot.

\begin{table}[h]
\begin{center}
\includegraphics[scale=1]{pictures/baratable3.pdf}
\end{center}
\label{bara}
\end{table}%


<<>>=
emph <- colSums(dictout) ## emphasis
propemph <- emph / sum(emph)*100 ## relative emphasis as a percentage
barplot(propemph)
@

Finally, let's revisit the floortime question but this time counting only vocabulary that Bara et al. thought was substantively relevant.
<<>>=
relevanttalk <- rowSums(dictout)
aggregate(relevanttalk ~ docvars(corp, "vote"), FUN=sum)
@
Now the balance of floor time spent saying `relevant' words is even more skewed, at around 3 to 1.


%' %' 
%' %' 
%' %' %\item 
%' %' For this exercise, we will work with the free, cross-platform, multilingual content analysis program \href{http://www.yoshikoder.org/}{Yoshikoder}. 
%' %' 
%' %' The Yoshikoder works with text documents, whether in plain ASCII, Unicode (e.g. UTF-8), or national encodings. You can construct, view, and save keywords-in-context. You can write content analysis dictionaries and apply them to documents. Yoshikoder provides summaries of documents, either as word frequency tables, according to a whole dictionary, or just some of its entries. You can also apply a dictionary analysis to the results of a concordance, which provides a way to study local word contexts. 
%' %' There is online help, and of course, for two days only in-person help too.
%' %' 
%' %' There are better commercial programs out there if you start doing this kind of things a lot, but this will do for now.
%' %' 
%' %' \subsection*{Abortion Politics in the UK}
%' %' 
%' %' %\item 
%' %' We start the exercise by replicating the dictionary-based content analysis in Bara, Weale, and Biquelet (2007). To do so, first download the dictionary file to somewhere convenient and then use 'Open Dictionary' from the Dictionary menu to open it.
%' %' 
%' %' Sometimes Windows likes to helpfully suffix these dictionary files with \texttt{.xml}\footnote{Annoying, isn't it?}.  Fear not -- it's still the right file.  Once loaded you can inspect the terms contained in the dictionary by clicking through the dictionary tree structure.
%' %' 
%' %' Next, load the fourteen legislative speeches from the UK House of Commons.  The `Add Documents' function on the `Documents' menu will do this.  Locate the files, select them all, and add them.  They will appear in a list on the right side of the application.  We've bundled each speaker's contributions together and included their final vote in the filename for convenience.  Yoshikoder allows you to identify the contexts where dictionary matches appear in the documents. To make them visible, you can make a concordance or highlight the entries in each document.
%' %' 
%' %' To create a dictionary report, select all the documents and the root of the dictionary tree.  From the `Report' menu select `Apply Dictionary' and then `Selected Documents'.  You can save the resulting dictionary report as an CSV or an Excel format file.  If you open the data you should have the raw materials to generate the table below.
%' %' 
%' %' \vspace{0.25cm}
%' %' 
%' %' \begin{table}[h]
%' %' \begin{center}
%' %' \includegraphics[scale=1]{baratable3.pdf}
%' %' \end{center}
%' %' \label{bara}
%' %' \end{table}%
%' %' 
%' %' The report has a distinctive format.  Documents are rows and all but the last column are counts of dictionary pattern matches in the categories of the dictionary.  The very first column is the root category which contains all the others, so the count here is the count of how many words matched any category at all.  The remaining columns are labeled according to their position in the tree.  The final column is a count of how many words were found in each document.  This gives a sense of what proportion of the word tokens apeeared in the dictionary.  
%' %' 
%' %' It also allows you calculate category rates per N words.  For example, from this final column we see that Mr William Deedes says 1478 words, of which 28 appeared in the `medical' category, so this is a rate of $28/1478 \times 1000 \approx$ 19 per thouand words.  The same category registers 50 words for Norman St John Stevas, but his rate is not much different at about 22 per thousand.
%' %' 
%' %' You should recover the category percentages to within about 1\% percent of the originals.  (Discrepancies are likely due to different software and -- regrettably -- are to be expected with text data.)  You can contruct the table from inside Excel, if you like that kind of thing.  
%' %' 
%' %' Alternatively, you can do bring the output file into R for further manipulation.  So let's do that.
%' %' \subsection*{Doing it in R}
%' %' 
%' %' For easy manipulation of Yoshikoder output in R it's convenient to load a few special functions.  They're on the web so you can do this by typing
%' %' <<>>=
%' %' source('http://dl.conjugateprior.org/iqmr/helpers.R')
%' %' @
%' %' at the beginning of your R session.
%' %' 
%' %' In the following we'll assume you saved the file as a CSV file (not an Excel file) and called it \texttt{ukabortiondebate}.
%' %' 
%' %' You can load this file using the \texttt{text\_ykd\_report} function that we just sourced.  Yoshikoder will have slightly renamed it by appending '-utf8' on the end to remind you what encoding it is in.  Here we use the `drop.total' argument to remove the final column.
%' %' <<>>=
%' %' report1 <- read_ykd_report('ukabortiondebate-utf8.csv', 
%' %'                            drop.total = TRUE)
%' %' @
%' %' by default the root category count will be dropped too, because we can reconstruct that by summing any row.
%' %' 
%' %' To recreate the table in R simply normalise the column sums
%' %' <<>>=
%' %' emph <- colSums(report1) ## emphasis
%' %' prop.emph <- emph / sum(emph)*100 ## relative emphasis as a percentage
%' %' @
%' %' 
%' %' Finally, we can plot the percentages
%' %' <<>>=
%' %' barplot(prop.emph,main="Categories in 1966 UK Abortion Debate")
%' %' @
%' %' 
%' %' 
%' %' \subsection*{Speaking Time}
%' %' 
%' %' In this debate `Mr Speaker' (Dr Horace King) promised in his opening words to give equal floor time to those opposed to the abortion bill and those in favour.  But did he?
%' %' 
%' %' Let us first make a variable representing whether a speaker eventually voted yes or no.  In the order of our data this is
%' %' <<>>=
%' %' vote <- c(rep(NA, 3), rep('no', 5), rep('yes', 16))
%' %' @
%' %' But now we need that the word total column that we dropped earlier.  Let's load in the data again and pull it out
%' %' <<>>=
%' %' report2 <- read_ykd_report('ukabortiondebate-utf8.csv')
%' %' tot <- report2$Total ## just the last column
%' %' @
%' %' Now, as it turns out, those that end up voting yes seem to get twice as much floor time
%' %' <<>>=
%' %' aggregate(tot ~ vote, FUN=sum) 
%' %' @
%' %' though that's because the few `no' voters spoke a lot
%' %' <<>>=
%' %' aggregate(tot ~ vote, FUN=mean)
%' %' @
%' %' 
%' 
%' \subsection*{Abortion Politics in the US}
%' 
%' Now we will compare this legislative debate with a similar one from the US Senate. This is the debate on the conference report for the Partial-Birth Abortion Ban Act of 2003. 
%' 
%' Import the US speech documents into Yoshikoder, re-do the dictionary analysis
%' <<>>=
%' us<-read_ykd_report("usabortion-utf8.csv",drop.total=TRUE)
%' @
%' 
%' Calculate per speaker proportions
%' <<>>=
%' usprop<-us/rowSums(us)
%' @
%' 
%' Select Democrats and Republicans 
%' <<>>=
%' dems<-usprop[grep("DEM",rownames(usprop)),]
%' reps<-usprop[grep("REP",rownames(usprop)),]
%' @
%' 
%' Calculate average proportions for Dems and Reps
%' <<>>=
%' demavg<-apply(dems,2,mean)
%' repavg<-apply(reps,2,mean)
%' @
%' 
%' Create one object
%' <<>>=
%' usres<-rbind(demavg,repavg)
%' rownames(usres)<-c("Dem","Rep")
%' @
%' and plot it
%' <<>>=
%' barplot(usres,beside=T,legend=T,ylim=c(0,0.4))
%' @
% 
% \begin{itemize}
% \item Do you find a similar or different distribution of keyword occurrences in the debate categories as defined by Bara et al.? 
% \item Can you find a partisan pattern in the data? To do this, compute averages of the category percentages separately for Democrats and Republicans.
% \item More generally, where do you see the benefits and/or limitations of applying this specific dictionary? Do you suspect that important terms are omitted? Notice that there is no ``gender politics'' category in the Bara et al. dictionary. Can you think of some terms that might reflect gender politics? Use Yoshikoder to add a gender category and appropriate patterns. Re-apply the dictionary  to the data. Do you find a partisan pattern?
% \end{itemize}

% \newpage
% 
% \subsection*{Looking ahead to Lecture 2}
% 
% This dictionary-based analysis relied on the intuitions of Bara et al. for its validity.  We can also consider how a completely unsupervised `topic model' would segment these documents into topics and assign them words.  You can find such a model at \url{http://dl.conjugateprior.org/jsLDA/jslda.html}.  
% 
% When you load that page it contains a collection of paragraphs from US State of the Union speeches.  In the field marked `Documents URL' you will see `sotu\_small.txt'.  This is the name of the source document that lives on the server.  
% 
% Replace that file by typing `abortion\_debate.txt' instead and press the `load' button. In a few moments the separate paragraphs of the abortion debate will appear.
% 
% To fit the topic model, set the number of topics and press the button on the top left of the page marked `Run 50 iterations'.  After these have completed the page will update with the new assignments of topics and words.  The split into paragraphs is necessary for the topic model because it relies on cross-document word variation to estimate the mixture of topics each document might contain.  Consequently we need to define document to be a smaller unit to provide enough of information.
% 
% Check the words that are most likely to have generated a topic. Do these topics make sense? Is the overlap with Bara's categories?


\end{document}



% \subsection*{Party Politics in the UK}
% 
% Finally, we look at the manifesto policy dictionary constructed by Laver and Garry (2000) and the development of economic policy positions, in particular. First, load the dictionary and the British party manifestos for the elections 1992 to 2010. 
% 
% Our goal is twofold: first, we would like to replicate the result that the Labour Party moved towards a neoliberal economic policy position in 1997 under Tony Blair, and want to examine how stable this move was in the subsequent elections. 
% 
% Open the dictionary file in Excel and calculate the economic policy positions for the parties as follows:
% \begin{equation*}
%   \text{ECON} = \frac{\text{ECON}_R - \text{ECON}_L}{\text{ECON}_R + \text{ECON}_L}
% \end{equation*}
% where $\text{ECON}_L$ is the total counts in category ``increase role of state in the economy'' and 
% $ECON_R$ is the total counts in category ``reduce role of state in the economy''. 
% \begin{itemize}
% \item Using Excel, plot the estimates for the parties over time (hint: you may need to re-arrange the data with parties in rows and years in columns).  Can you replicate the rightward move of Labour in 1997? Does Labour stay on the right in subsequent years?
% \item Following the election in 2010, none of the parties could muster a majority in parliament. Subsequently, the Conservative and Liberal Democratic party formed a coalition government, and Labour went into the opposition. Based on your estimates of economic policy positions derived from the dictionary counts, does this coalition appear sensible from the Liberal Democratic party's perspective?
% \end{itemize}

%\end{enumerate}
