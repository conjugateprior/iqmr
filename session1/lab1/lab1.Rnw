\documentclass[10pt,a4paper]{article}
%\usepackage[utf8]{inputenc}
\usepackage[charter]{mathdesign}
\usepackage[scaled=.95]{inconsolata}
\usepackage[margin=1.1in]{geometry}
\usepackage{color}

\usepackage{hyperref}
\usepackage{dcolumn,booktabs} %% for memisc
\usepackage{graphicx}
\usepackage{amsmath}

\definecolor{darkblue}{rgb}{0,0,.6} % not really
\definecolor{other}{rgb}{0,0,.5}
\hypersetup{colorlinks=true, linkcolor=darkblue, citecolor=darkblue,
	filecolor=darkblue,urlcolor=other}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}


\author{Will Lowe}
\title{IQMR: Lab 1}

\date{}

\begin{document}

\maketitle

<<include=FALSE>>=
knitr::opts_chunk$set(error=FALSE,
                      comment='',
                      size="small")
options(width=100)
@


\section{Introduction to \texttt{quanteda}}

\subsection*{Building a corpus}

Let's start by reading in the data from the abortion debate analyzed by Bara et al.  I've concatenated each speaker's contributions into a single file.
(This is certainly not the only way to think about analyzing this data, but it's what Bara et al. did.)

First load the package
<<>>=
library(quanteda) # for general text analysis
library(readtext) # for reading in all kinds of files
@
then read in some text files and make a \texttt{corpus} from them
<<>>=
txts <- readtext("data/abortion-debate-by-speaker/*.txt")
corp <- corpus(txts)
@
Corpora get big quickly, so most functions in the package will not show you all the contents of any object.  Call \texttt{summary} to get a view of the new corpus object.

It's helpful to add some metadata to the documents, so we can subset them.  Here we'll record the vote of each speaker.
<<>>=
vote <- c("abs", "abs", "abs", "no", "no", "no", "no",
          "no", "yes", "yes", "yes", "yes", "yes", "yes",
          "yes", "yes", "yes", "yes", "yes", "yes", "yes",
          "yes", "yes", "yes")
docvars(corp, "vote") <- vote
summary(corp)
@
where \texttt{docvars} adds document specific metadata, here the speaker's vote.

The number of words spoken by each speaker might be useful, so we'll add that too, taking it straight out of the the summary
<<>>=
docvars(corp, "words_spoken") <- summary(corp)$Tokens
@

If we want to deal with just the docvars we can get them all as a data.frame like this
<<>>=
dvars <- docvars(corp)
@

Conversely, if we want to the texts in this corpus object without the docvars we use
<<eval=FALSE>>=
texts(corp) # caution, quite slow to show...
@
To just get the contributions of the (magnificently named) Mr Norman St John-Stevas, we can index into it.  He's the 5th `document'.
<<eval=FALSE>>=
texts(corp)[5]
@
To see just a few speakers, e.g. the ones that voted against, we can use \texttt{subset}
<<>>=
nocorp <- corpus_subset(corp, vote == "no")
summary(nocorp)
@
or we could use it to remove very short speeches
<<eval=FALSE>>=
longcorp <- corpus_subset(corp, words_spoken > 50)
@
(though actually we could use the more flexible \texttt{corpus\_trim} for this).

\subsection*{Exploring text corpora}

First a very gross view of how these legislator are talking in terms of speech complexity.  We'll use an old readabilty measure and a measure of lexical diversity.
<<>>=
readability <- textstat_readability(corp, "Flesch.Kincaid")
range(readability) # who is so unreadable?
which(readability > 20) # looking at you Mr Mendelson
texts(corp)[19] # yes, I mean no, I mean, what?
@

Let's explore a little more by looking for the key terms  in play.  One way to do this is to look for collocations.  We'll look at the top few according to a  `pointwise mutual information' measure (\texttt{pmi}).  The function operates on the tokens of the corpus, so we extract them first
<<>>=
toks <- tokens(corp)
head(textstat_collocations(toks, method="pmi"), 20)
@
This works better, the larger the corpus. You can tweak the results by changing the statistic used to score word pairs.

If you're feeling adventurous within quanteda, here's a rather complex example that tries to find better ones
<<>>=
toks2 <- tokens(corpus_segment(corp, what = "sentence")) # split to sentences
toks2 <- tokens_select(toks2, stopwords(),
                       "remove", padding = TRUE) # remove stopwords
toks2 <- tokens_select(toks2, "[a-z]+", valuetype="regex",
                       case_insensitive = FALSE,
                       padding = TRUE) #
seqs <- textstat_collocations(toks2, method = "bj_uni")
@
You can examine \texttt{seqs} at your leisure.

If we're really serious about collocation hunting, it's probably best to use a dedicated package, e.g. \texttt{phrasemachine}.

But let's get a bit more confirmatory in our text analysis

\subsection*{Keywords in context}

Since this is an abortion debate, let's see the honorable folk talk about mothers and babies. We'll use the `keyword in context' function
<<eval=FALSE>>=
kwic(corp, "mother*")
@
we might benefit from a bit more local context, so maybe set the window a bit wider  Here are the babies
<<eval=FALSE>>=
kwic(corp, "babi*", window=10)
@
you may need to expand your window a bit to see these properly.

Perhaps oddly, there is much less talk of babies than of mothers.
In this debate, the other major actors are doctors and their professional association, which you can investigate the same way.

The output of kwic is simply a data.frame, so one thing that's often useful is to treat the left and right sides of the kwic as a document (about babies), e.g. like this:
<<>>=
babes <- kwic(corp, "babi*", window=10)
txt <- paste(babes$pre, babes$post,
             collapse=" ") # make one big string
@
This constructed document contains, but definition, all the ways the term was used in the corpus, so you can then examine what sorts of words tend to be used around it, e.g. by using a content analysis dictionary.\footnote{As far as I know this was pioneered by the Yoshikoder software, and bears some obvious similarities to current word-embedding analyses.}

Now let's get even more confirmatory, and apply the mapping between words and topics described by Bara et al.'s dictionary to this corpus, in order to replicate their first analysis.

Our first stem will be to create a document feature matrix (\texttt{dfm}), after which dictionary application is straightforward.  But since lots of models require a dfm, we'll linger a little on the steps of the process.

\subsection*{Constructing a document feature matrix}

quanteda makes a basic dfm quite straightforward
<<>>=
corpdfm <- dfm(corp) # lowercases by default, but nothing more
dim(corpdfm)
featnames(corpdfm)[1:40] # really just colnames
@
But let's remove some things that aren't (currently) of interest to us
<<>>=
corpdfm <- dfm(corp, remove=stopwords(), remove_punct=TRUE,
               remove_numbers=TRUE)
dim(corpdfm) # a little smaller
featnames(corpdfm)[1:40]
@
We \textit{could} also stem
<<>>=
stemdfm <- dfm(corp, remove=stopwords(), remove_punct=TRUE,
               remove_numbers=TRUE, stem=TRUE)
dim(stemdfm) # about 1000 fewer 'word's
featnames(stemdfm)[1:40]
@
but our dictionary entries aren't stemmed, so let's save \texttt{stemdfm} for later.

For modeling, we'll often want to remove the low frequency and idiosyncratic words
<<>>=
smallcorpdfm <- dfm_trim(corpdfm, min_count=5, min_docfreq=5)
dim(smallcorpdfm2)
@
where \texttt{min\_count} removes any word that occurs less than 5 times and m\texttt{in\_docfreq} removes any words that occurs any number of times but  in fewer than 5 documents.  That makes things a fair bit smaller.  But again, we don't want to miss dictionary entries.

There's also a \texttt{wordcloud} function for viewing the the document feature matrix, but we won't use it because wordclouds are silly.

\subsection*{Answering questions with text}

In the debate the Speaker, Mr Horace King, said he would try to give equal time to both sides of the debate.  (You can read the original debate as \texttt{data/abortion-debate-hansard.html}).  Did it happen this way?

%It's hard to know whether the debate was persuasive since we do not know the speakers prior beliefs (though we could find out from their previous debates) so let us assume that there was no substantial persuasion.
We'll assume that no speaker spoke particularly slowly.  These imply that we can proxy speaking time with number of words said.
<<>>=
aggregate(words_spoken ~ vote, data=docvars(corp), FUN=sum)
@
It appears that floor time was about two to one yes to no voters.  However, individual no voters did tend to get more time each
<<>>=
aggregate(words_spoken ~ vote, data=docvars(corp), FUN=median)
@

\subsection*{Applying a content analysis dictionary}

Let's turn to the content analysis dictionary that Bara used.
A content analysis dictionary in \texttt{quanteda} terms can be made out of a list of vectors of words, like this:
<<>>=
dictionary(list(medics=c("doctor", "medical", "hospital"),
                mothers=c("mother", "parents")))
@
or imported in the format of some other content analysis program.\footnote{Quanteda can deal with dictionaries from Wordstat, Yoshikoder, and LIWC}.  We'll read the dictionary in Yoshikoder format:
<<>>=
baradic <- dictionary(file="data/2007_abortion_dictionary.ykd")
@

\subsection*{Replicating a little bit of Bara}

With dictionary in hand we can now go \textit{category} counting rather than word counting
<<>>=
baradfm <- dfm(corp, dictionary=bara)
@
Since this output is not absolutely massive
<<>>=
dim(baradfm)
@
let's force it into a regular R matrix to take a look at the whole thing without being swamped in elements
<<>>=
dictout <- as.matrix(baradfm)
dictout
@
And recreate some of Bara et al.'s Table 3, only as a bar plot.

\begin{table}[h]
\begin{center}
\includegraphics[scale=1]{pictures/baratable3.pdf}
\end{center}
\label{bara}
\end{table}%


<<>>=
emph <- colSums(dictout) ## emphasis
propemph <- emph / sum(emph)*100 ## relative emphasis as a percentage
barplot(propemph)
@

Finally, let's revisit the floortime question but this time counting only vocabulary that Bara et al. thought was substantively relevant.
<<>>=
relevanttalk <- rowSums(dictout)
aggregate(relevanttalk ~ docvars(corp, "vote"), FUN=sum)
@
Now the balance of floor time spent saying `relevant' words is even more skewed, at around 3 to 1.


%' %'
%' %'
%' %' %\item
%' %' For this exercise, we will work with the free, cross-platform, multilingual content analysis program \href{http://www.yoshikoder.org/}{Yoshikoder}.
%' %'
%' %' The Yoshikoder works with text documents, whether in plain ASCII, Unicode (e.g. UTF-8), or national encodings. You can construct, view, and save keywords-in-context. You can write content analysis dictionaries and apply them to documents. Yoshikoder provides summaries of documents, either as word frequency tables, according to a whole dictionary, or just some of its entries. You can also apply a dictionary analysis to the results of a concordance, which provides a way to study local word contexts.
%' %' There is online help, and of course, for two days only in-person help too.
%' %'
%' %' There are better commercial programs out there if you start doing this kind of things a lot, but this will do for now.
%' %'
%' %' \subsection*{Abortion Politics in the UK}
%' %'
%' %' %\item
%' %' We start the exercise by replicating the dictionary-based content analysis in Bara, Weale, and Biquelet (2007). To do so, first download the dictionary file to somewhere convenient and then use 'Open Dictionary' from the Dictionary menu to open it.
%' %'
%' %' Sometimes Windows likes to helpfully suffix these dictionary files with \texttt{.xml}\footnote{Annoying, isn't it?}.  Fear not -- it's still the right file.  Once loaded you can inspect the terms contained in the dictionary by clicking through the dictionary tree structure.
%' %'
%' %' Next, load the fourteen legislative speeches from the UK House of Commons.  The `Add Documents' function on the `Documents' menu will do this.  Locate the files, select them all, and add them.  They will appear in a list on the right side of the application.  We've bundled each speaker's contributions together and included their final vote in the filename for convenience.  Yoshikoder allows you to identify the contexts where dictionary matches appear in the documents. To make them visible, you can make a concordance or highlight the entries in each document.
%' %'
%' %' To create a dictionary report, select all the documents and the root of the dictionary tree.  From the `Report' menu select `Apply Dictionary' and then `Selected Documents'.  You can save the resulting dictionary report as an CSV or an Excel format file.  If you open the data you should have the raw materials to generate the table below.
%' %'
%' %' \vspace{0.25cm}
%' %'
%' %' \begin{table}[h]
%' %' \begin{center}
%' %' \includegraphics[scale=1]{baratable3.pdf}
%' %' \end{center}
%' %' \label{bara}
%' %' \end{table}%
%' %'
%' %' The report has a distinctive format.  Documents are rows and all but the last column are counts of dictionary pattern matches in the categories of the dictionary.  The very first column is the root category which contains all the others, so the count here is the count of how many words matched any category at all.  The remaining columns are labeled according to their position in the tree.  The final column is a count of how many words were found in each document.  This gives a sense of what proportion of the word tokens apeeared in the dictionary.
%' %'
%' %' It also allows you calculate category rates per N words.  For example, from this final column we see that Mr William Deedes says 1478 words, of which 28 appeared in the `medical' category, so this is a rate of $28/1478 \times 1000 \approx$ 19 per thouand words.  The same category registers 50 words for Norman St John Stevas, but his rate is not much different at about 22 per thousand.
%' %'
%' %' You should recover the category percentages to within about 1\% percent of the originals.  (Discrepancies are likely due to different software and -- regrettably -- are to be expected with text data.)  You can contruct the table from inside Excel, if you like that kind of thing.
%' %'
%' %' Alternatively, you can do bring the output file into R for further manipulation.  So let's do that.
%' %' \subsection*{Doing it in R}
%' %'
%' %' For easy manipulation of Yoshikoder output in R it's convenient to load a few special functions.  They're on the web so you can do this by typing
%' %' <<>>=
%' %' source('http://dl.conjugateprior.org/iqmr/helpers.R')
%' %' @
%' %' at the beginning of your R session.
%' %'
%' %' In the following we'll assume you saved the file as a CSV file (not an Excel file) and called it \texttt{ukabortiondebate}.
%' %'
%' %' You can load this file using the \texttt{text\_ykd\_report} function that we just sourced.  Yoshikoder will have slightly renamed it by appending '-utf8' on the end to remind you what encoding it is in.  Here we use the `drop.total' argument to remove the final column.
%' %' <<>>=
%' %' report1 <- read_ykd_report('ukabortiondebate-utf8.csv',
%' %'                            drop.total = TRUE)
%' %' @
%' %' by default the root category count will be dropped too, because we can reconstruct that by summing any row.
%' %'
%' %' To recreate the table in R simply normalise the column sums
%' %' <<>>=
%' %' emph <- colSums(report1) ## emphasis
%' %' prop.emph <- emph / sum(emph)*100 ## relative emphasis as a percentage
%' %' @
%' %'
%' %' Finally, we can plot the percentages
%' %' <<>>=
%' %' barplot(prop.emph,main="Categories in 1966 UK Abortion Debate")
%' %' @
%' %'
%' %'
%' %' \subsection*{Speaking Time}
%' %'
%' %' In this debate `Mr Speaker' (Dr Horace King) promised in his opening words to give equal floor time to those opposed to the abortion bill and those in favour.  But did he?
%' %'
%' %' Let us first make a variable representing whether a speaker eventually voted yes or no.  In the order of our data this is
%' %' <<>>=
%' %' vote <- c(rep(NA, 3), rep('no', 5), rep('yes', 16))
%' %' @
%' %' But now we need that the word total column that we dropped earlier.  Let's load in the data again and pull it out
%' %' <<>>=
%' %' report2 <- read_ykd_report('ukabortiondebate-utf8.csv')
%' %' tot <- report2$Total ## just the last column
%' %' @
%' %' Now, as it turns out, those that end up voting yes seem to get twice as much floor time
%' %' <<>>=
%' %' aggregate(tot ~ vote, FUN=sum)
%' %' @
%' %' though that's because the few `no' voters spoke a lot
%' %' <<>>=
%' %' aggregate(tot ~ vote, FUN=mean)
%' %' @
%' %'
%'
%' \subsection*{Abortion Politics in the US}
%'
%' Now we will compare this legislative debate with a similar one from the US Senate. This is the debate on the conference report for the Partial-Birth Abortion Ban Act of 2003.
%'
%' Import the US speech documents into Yoshikoder, re-do the dictionary analysis
%' <<>>=
%' us<-read_ykd_report("usabortion-utf8.csv",drop.total=TRUE)
%' @
%'
%' Calculate per speaker proportions
%' <<>>=
%' usprop<-us/rowSums(us)
%' @
%'
%' Select Democrats and Republicans
%' <<>>=
%' dems<-usprop[grep("DEM",rownames(usprop)),]
%' reps<-usprop[grep("REP",rownames(usprop)),]
%' @
%'
%' Calculate average proportions for Dems and Reps
%' <<>>=
%' demavg<-apply(dems,2,mean)
%' repavg<-apply(reps,2,mean)
%' @
%'
%' Create one object
%' <<>>=
%' usres<-rbind(demavg,repavg)
%' rownames(usres)<-c("Dem","Rep")
%' @
%' and plot it
%' <<>>=
%' barplot(usres,beside=T,legend=T,ylim=c(0,0.4))
%' @
%
% \begin{itemize}
% \item Do you find a similar or different distribution of keyword occurrences in the debate categories as defined by Bara et al.?
% \item Can you find a partisan pattern in the data? To do this, compute averages of the category percentages separately for Democrats and Republicans.
% \item More generally, where do you see the benefits and/or limitations of applying this specific dictionary? Do you suspect that important terms are omitted? Notice that there is no ``gender politics'' category in the Bara et al. dictionary. Can you think of some terms that might reflect gender politics? Use Yoshikoder to add a gender category and appropriate patterns. Re-apply the dictionary  to the data. Do you find a partisan pattern?
% \end{itemize}

% \newpage
%
% \subsection*{Looking ahead to Lecture 2}
%
% This dictionary-based analysis relied on the intuitions of Bara et al. for its validity.  We can also consider how a completely unsupervised `topic model' would segment these documents into topics and assign them words.  You can find such a model at \url{http://dl.conjugateprior.org/jsLDA/jslda.html}.
%
% When you load that page it contains a collection of paragraphs from US State of the Union speeches.  In the field marked `Documents URL' you will see `sotu\_small.txt'.  This is the name of the source document that lives on the server.
%
% Replace that file by typing `abortion\_debate.txt' instead and press the `load' button. In a few moments the separate paragraphs of the abortion debate will appear.
%
% To fit the topic model, set the number of topics and press the button on the top left of the page marked `Run 50 iterations'.  After these have completed the page will update with the new assignments of topics and words.  The split into paragraphs is necessary for the topic model because it relies on cross-document word variation to estimate the mixture of topics each document might contain.  Consequently we need to define document to be a smaller unit to provide enough of information.
%
% Check the words that are most likely to have generated a topic. Do these topics make sense? Is the overlap with Bara's categories?


\end{document}



% \subsection*{Party Politics in the UK}
%
% Finally, we look at the manifesto policy dictionary constructed by Laver and Garry (2000) and the development of economic policy positions, in particular. First, load the dictionary and the British party manifestos for the elections 1992 to 2010.
%
% Our goal is twofold: first, we would like to replicate the result that the Labour Party moved towards a neoliberal economic policy position in 1997 under Tony Blair, and want to examine how stable this move was in the subsequent elections.
%
% Open the dictionary file in Excel and calculate the economic policy positions for the parties as follows:
% \begin{equation*}
%   \text{ECON} = \frac{\text{ECON}_R - \text{ECON}_L}{\text{ECON}_R + \text{ECON}_L}
% \end{equation*}
% where $\text{ECON}_L$ is the total counts in category ``increase role of state in the economy'' and
% $ECON_R$ is the total counts in category ``reduce role of state in the economy''.
% \begin{itemize}
% \item Using Excel, plot the estimates for the parties over time (hint: you may need to re-arrange the data with parties in rows and years in columns).  Can you replicate the rightward move of Labour in 1997? Does Labour stay on the right in subsequent years?
% \item Following the election in 2010, none of the parties could muster a majority in parliament. Subsequently, the Conservative and Liberal Democratic party formed a coalition government, and Labour went into the opposition. Based on your estimates of economic policy positions derived from the dictionary counts, does this coalition appear sensible from the Liberal Democratic party's perspective?
% \end{itemize}

%\end{enumerate}
