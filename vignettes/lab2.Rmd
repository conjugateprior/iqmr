---
title: "Lab 2"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_width: 7
    fig_height: 10 
vignette: >
  %\VignetteIndexEntry{Lab 2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this lab we're going to investigate a different debate on abortion law, 
this time in the United States.  The debate occurred on 
October 21, 2003 as the 
last stage of the passage of the 
['Partial-Birth Abortion Ban Act of 2003'](https://www.govtrack.us/congress/bills/108/s3)
that banned specific forms of abortion. 

There are 23 speakers and we will be interested in simultaneously discovering the 
main topics of the debate and decomposing each speaker's (and party's) 
patterns of emphasis over those topics.
We'll use a representation of the debate that bundles all speaker contributions
together, scraped from the plain-ish text versions offered by 
`congress.gov`.  The process of extracting this information is detailed in 
another lab.

However, we will usually need more than 23 documents to get a
got topic model going.  Accordingly we'll treat a smaller unit as our documents
- it will be whatever ther Congressional Record thinks should be a paragraph.

Here are the raw materials
```{r}
library(iqmr)

data("df_pba_debate_by_speaker")
names(df_pba_debate_by_speaker)
```
we'll make a corpus from them, using the speaker's name as a document identifier
and we'll also explicitly add the speaker names as a 
document variable because we'll want to refer to them later.
```{r}
corp <- corpus(df_pba_debate_by_speaker, 
               text_field = "contributions", 
               docid_field = "speaker")
docvars(corp, "speaker") <- df_pba_debate_by_speaker$speaker # add speaker field
summary(corp)
```

Now we'll do the switch to paragraphs
```{r}
para_corp <- corpus_reshape(corp, to = "paragraphs") 
head(summary(para_corp))
```
Notice that the texts now have a numerical identifier added and the other 
document variables have been duplicated appropriately.

Although contributions are probably too big, ironically paragraphs run the risk 
of being too small (or even
disappearing if the paragraph spotting algorithm is imperfect).  Let's 
check
```{r}
table(ntoken(para_corp)) # 15 documents with, err, no words
```
so we'll trim these tiny documents.  Then we'll make a document term matrix
for the topic model to work with
```{r}
para_corp <- corpus_subset(para_corp, ntoken(para_corp) > 2)

para_dfm <- dfm(para_corp,
  remove_punct = TRUE,
  remove = stopwords(), 
  remove_numbers = TRUE)
```

To start with we'll fit a 10 topic topic model using the `stm` package
```{r, eval = FALSE}
library(stm)

mod <- stm(para_dfm, K = 10, seed = 12345)
```
<!-- I don't want to verbose = FALSE but I want that effect in the document-->
```{r, include = FALSE}
library(stm)

mod <- stm(para_dfm, K = 10, seed = 12345)
```
Don't forget to set the seed so others can replicate your work.  

Now let's take a look at the topic word associations we've estimated.
```{r}
labelTopics(mod)
```

For putting in online appendices and showing to your co-authors, a plot is
often more compelling
```{r}
plot(mod, type = "labels", labeltype = "prob") # or frex, lift, score
```
You may need to pop out the window to make this big enough to view.

To get a better sense of a topic it's sometimes helpful to look at a sample
of documents a lot of whose tokens were assigned to it.  Here
we find documents (for us paragraphs) that have a lot of topic 1.
```{r}
findThoughts(mod, texts = texts(para_corp), topics = 1)
```
This should be mostly about issues of constitutionality. Try a few others 
(but be aware that topic 8 is deliberately gruesome).

## Model checking

We would be remiss not to run some basic model diagnostics.  Here are three, 
conveniently built into the package.  Outside the lab, we'd spend
a lot more time on this part!

First, we'd prefer it if words were not exclusively generated by one topic
(we're trying to abstract up from the words after all)
```{r}
checkBeta(mod)$problemWords # we'll look at the 'problem words field'
```

On the other hand we'd like a certain amount of exclusivity to the topics.  
Exclusivity here means how much each each topic has its own
vocabulary not used by other topics
```{r}
dotchart(exclusivity(mod), labels = 1:10)
```

We'd also like the topics to be coherent.  If the topics are in fact semantically 
coherent, one thing we'd expect is that words a topic generates
should co-occur often in the same document.  One measure of this is  
```{r}
cohere <- semanticCoherence(mod, para_dfm)
dotchart(cohere, labels = 1:10)
```
and it looks like we might want to check topics 2 and 7.  Typically semantically
'incoherent' vocabulary are stop words (which we removed), 
and procedural vocabulary. Here it seems like they are just more general terms 
in the debate topic that everyone is happy to use.

## Predicting topic prevalences

We may reasonably expect that both parties and speakers would prefer to emphasise
a different set of topics.  We have two options to connect speaker and party
identifiers to topics.

First, we can manually extract the topic proportions
from the model and use parts of them as a dependent variable.
The topic proportions for each document are inside the model object
```{r}
head(mod$theta)
```
We can then tie these to the document variables and get a high level 
view of who talks about what.  Who tends to use topic 1?
```{r}
df <- data.frame(topic1 = mod$theta[,1], docvars(para_dfm))
head(df)
```
Fit a simple random effects model in which speakers are a draw from a 
population of legislators
```{r}
library(lme4)
library(lattice)

lmer_topic1 <- lmer(topic1 ~ (1 | speaker), data = df)
dotplot(ranef(lmer_topic1, condVar = TRUE))
```
Apparently Cantell and Feinstein are the topic 1 users here, 
possibly also Mikulski and Nelso, but we're fairly unsure about that.

## Let STM do both steps

We can also fit the topic model taking into account that we think we 
know what factors might predict different topic prevalence.  This time, 
let's look at how party relates to topic prevalence.

We need to add two bits of information to the model specification: the
the document variables we think are predictive, and the functional form
of the relationship between them and topics.  The document variables
come in as a data argument and the functional form is specified as the right
hand side of a formula.  To say that party affiliation predicts topic prevalence
we say `prevalence = ~ party`, and fit the model as before
```{r, eval = FALSE}
mod2 <- stm(para_dfm, K = 10, 
           prevalence = ~ party, data = docvars(para_dfm),
           seed = 12345)
```
<!-- I don't want to verbose = FALSE but I want that effect in this document-->
```{r, include = FALSE}
mod2 <- stm(para_dfm, K = 10, 
           prevalence = ~ party, data = docvars(para_dfm),
           seed = 12345)
```

If we are interested in topics (constitutional) 1 and (gruesome) 8, we can focus
in on their relationship to party like this:
```{r}
gg <- estimateEffect(c(1,8) ~ party, mod2, docvars(para_dfm))
summary(gg)
```
Note here that the baseline topic prevalence is Allard (R).  By changing the
numbers in the 'dependent variable' part of the formula first argument
we can look at as many of the topics as we like.

As always, a plot is nicer
```{r}
plot(gg, "party", labeltype = "prob", model = mod2)
```
From this we can see that Republican speakers use topic 1 somewhat less
than Democrats, but topic 8 *much* more.  

If you'd like to know whether this is a general Republican thing or
just a quirk of one or two speakers, adjust the model above to use speaker as 
the predictor and follow the steps above.

In general you can use any specification you like in the formula, so you 
don't have to choose individual covariates, as we did here.  In particular
it's often nice to 'control' for time in a smooth fashion.  For example if 
you have a year variable and enough years you might use `party + s(year)` 
as the formula, which would allow topic prevalences to move smoothly over time, 
and the party 'effect' to be estimated against this background.

Note also that formulae need to know about nesting in data, so `party/speaker` 
would be the way to express that speakers are in one party only.

<!-- plot(gg, "party", labeltype = "prob", model = mod2) -->

<!-- summary(lm(I(mod$theta[,8]) ~ I(docvars(para_dfm, "party")))) -->

<!-- dim(mod$beta$logbeta[[1]]) # log probability for 10 topics x 3920 words -->



<!-- ## Text Classification: What's in the New York Times? -->

<!-- In this lab we will be using a corpus of article titles and description from -->
<!-- the New York Times.  The classification task is to determine which of 27 subject categories a story is from.  You can find a list of these categories in the accompanying document. -->

<!-- We begin by loading the library we will be using for classification  -->
<!-- <<loading,results='hide',warning=FALSE,message=FALSE>>= -->
<!-- library(RTextTools) ## may need to install.packages('RTextTools') first -->
<!-- load('nytimes-sample.RData') -->
<!-- @ -->
<!-- If that didn't work you probably don't have the library installed, so install it by typing  -->
<!-- The documents we wil be using are bundled with the library so we load them directly -->

<!-- This is a \texttt{data.matrix}, so we first see what sorts of variables we have available to us -->
<!-- <<variables,comment=''>>= -->
<!-- colnames(nytimes.sample) -->
<!-- @ -->
<!-- We will be most interested in the text fields \texttt{Subject} and \texttt{Title} and the numerical subject category label \texttt{Topic.Code}, which is conveniently translated as \texttt{Topic}. -->
<!-- For convenience the code to topic mapping is shown in Table~\ref{codes}. -->

<!-- <<code-table,echo=FALSE,results='asis'>>= -->
<!-- codes <- read.csv('topics-nyt.csv', row.names=1) -->
<!-- library(xtable) -->
<!-- xtable(codes, caption='New York Times Index topic codes and their descriptive names', -->
<!-- label='codes') -->
<!-- @ -->

<!-- Let's take a look at the first five `documents' (which are essentially NY Times headlines). Which topic would you assign to these? -->
<!-- <<preview,comment=''>>= -->
<!-- head(nytimes.sample[,c('Title', 'Subject')]) -->
<!-- @ -->
<!-- You can see whether your coding corresponds to the actual labels human coders assigned them -->
<!-- <<preview2,comment=''>>= -->
<!-- head(nytimes.sample[,c('Topic','Topic.Code')]) -->
<!-- @ -->
<!-- In order to build a document classifier we first  -->
<!-- transform this nice readable dataset into a sparse matrix of word counts. -->

<!-- As part of the process of doing that we'll take the opportunity to throw out number words, reduce  -->
<!-- to word stems, and weight the resulting word counts using a standard term  -->
<!-- frequency-inverse document frequency (tfidf) transformation.   -->
<!-- <<make-matrix>>= -->
<!-- matrix <- create_matrix(cbind(nytimes.sample$Title,nytimes.sample$Subject), -->
<!--                         language="english", -->
<!--                         removeNumbers=TRUE,  -->
<!--                         stemWords=TRUE) -->
<!-- @ -->
<!-- where the \texttt{cbind} function ties together two columns from \texttt{nytimes.sample}. -->
<!-- We handed in the \texttt{Title} and \texttt{Subject} fields together -->
<!-- because we expect that they both provide information about the topic code.  We -->
<!-- could have done more (or less) pre-processing as we counted and transformed the -->
<!-- words.  See the help for \texttt{create\_matrix} for details. -->

<!-- Now that we have a suitable representation for our documents we create a container object -->
<!-- to hold both them and their category labels.  We'll use the container to train and -->
<!-- also to test fitted classifiers -->
<!-- <<make-corpus>>= -->
<!-- set.seed(1234) ## so your cases will be mine -->
<!-- training.cases <- 1:2000 -->
<!-- test.cases <- 2001:3104 -->
<!-- corpus <- create_container(matrix, nytimes.sample$Topic.Code,  -->
<!--                            trainSize=training.cases,  -->
<!--                            testSize=test.cases,  -->
<!--                            virgin=FALSE) -->
<!--   @ -->
<!--  Here we specify the matrix we just made out of the documents as the first argument, -->
<!--  the column of category labels as the second, and then a division into training -->
<!--  and test documents.  The not-quite-accurately named \texttt{trainSize} parameter -->
<!--  contains the row numbers that we want to use to fit the classifiers, here the  -->
<!--  first thousand documents.  The similarly inaccurately names \texttt{testSize}  -->
<!--  parameters indicates that we want to test them on the remaining documents.  The -->
<!--  bizarrely named \texttt{virgin} parameter is false, indicating that we actually -->
<!--  do know the correct labels for the test documents.  This will not, of  -->
<!--  course, always be true. -->

<!--  The library is set up to run lots of classifiers and have them vote on each test  -->
<!--  document.  But for now let's just train one, regularised  -->
<!--  logistic regression which computational linguists (for reasons best known -->
<!--  to themselves) refer to as the maximum entropy classifier or `maxent'. -->
<!--  <<train>>= -->
<!--  models <- train_models(corpus, algorithms=c("MAXENT")) -->
<!--  @ -->
<!--  Normally in R we would want to summarise a fitted model.  In this case however, the  -->
<!--  parameters are unlikely to be particularly illuminating (and there are several -->
<!--  hundred of them), so no such methods are provided. -->

<!--  If we had wanted to train more or different models, we would have had to put their  -->
<!--  names in the \texttt{algorithms} parameter.  The options are  -->
<!--  <<algorithms,comment=''>>= -->
<!--  print_algorithms() -->
<!--  @ -->
<!--  and you can spend a happy afternoon reading about them on the internet. -->

<!--  Now that our single classifier is trained we would like to see how well it works -->
<!--  <<classify>>= -->
<!--  results <- classify_models(corpus, models) -->
<!--  @ -->
<!--  This particular function hides a fair amount of detail about the model, which is  -->
<!--  usually what we want.  Looking at the first few results  -->
<!--  <<results,comment=''>>= -->
<!--  head(results) -->
<!--  @ -->
<!--  we see the topic in the first column and the probability that the classifier -->
<!--  assigned this topic in the second column.  Despite more than 20 possible categories this model -->
<!--  seems rather confident.  This is not necessarily a good thing.   -->

<!--  As a first look at the  -->
<!--  performance of the model, let us see what how likely it is -->
<!--  to put a document in the right category, and then examine what sorts of errors are made. -->

<!--  The library comes with various forms of `analytics',  -->
<!--  that is: performance summaries.  Let's  -->
<!--  compute all of them for the corpus we trained with and the results we got -->
<!--  <<analytics>>= -->
<!--  analytics <- create_analytics(corpus, results)  -->
<!--  @ -->
<!--  The analytics object is a vast sprawling thing that you can read about on its help page. -->

<!--  One summary we can make from it is the probability that the classifier puts a  -->
<!--  document in the right category (recall), which we might want to break down by category.  Another is the -->
<!--  probability that a document is in a category when the model says it is (precision). -->
<!--  Actually we want these things often enough that they get their own function, the idiosyncratically punctuated  -->
<!--  <<precision-recall>>= -->
<!--  pr <- create_precisionRecallSummary(corpus, results) -->
<!--  @ -->
<!--  which we show in Table~\ref{pr}, including the F-measure which combines precision and recall.  The is useful if you like reducing vast amounts of disparate information to single number summaries.  We don't, so we'll ignore it. -->
<!-- %  -->

<!--  <<pr-table,echo=FALSE,results='asis'>>= -->
<!--  colnames(pr) <- c('precision', 'recall', 'F') ## work around underscore bug in memisc's toLatex  -->
<!-- xtable(pr, digits=2, caption="Precision, recall and the F measure for classifier performance", label="pr") -->
<!--  @ -->

<!-- What difference do these errors make?  One way to check this we can make a time series of Defense/International Affairs stories as a proportion of all New York Times stories and compare it to the time series we would have gotten if we had used our classifier's decisions instead. -->

<!-- First, let's pull out the rows corresponding to test documents from the original -->
<!-- data set -->
<!-- <<truecats>>= -->
<!-- testdata <- nytimes.sample[test.cases,] -->
<!-- @ -->
<!-- and add the model's predictions about their topic as a new column -->
<!-- <<modelests>>= -->
<!-- testdata$Model.Topic.Code <- results[,1] ## just the topic code  -->
<!-- @ -->
<!-- To make our time series we have to aggregate a bit, so let's add a monthly indicator variable.  This will allow us to aggregate documents that appear within the same month. This smooths the  -->
<!-- data a bit. -->
<!-- <<make-cut>>= -->
<!-- testdata$month <- cut(testdata$Date, 'month') -->
<!-- bigt <- with(testdata, table(Topic.Code, month)) -->
<!-- @ -->
<!-- The bigt cross tabulation has months as columns and aggregated topic code counts as rows.  It's -->
<!-- a bit unwieldy to look at, so let us summarise the amount of attention given to 'International Affairs' -->
<!-- and `Defense' (codes 19 and 16) as a proportion of all the different sorts of news. -->
<!-- <<crosstab2>>= -->
<!-- all.news <- colSums(bigt) -->
<!-- foreigners <- colSums(bigt[c('19','16'),])  ## careful with the quotes -->
<!-- time <- as.Date(colnames(bigt)) ## turn our indicator back into date -->
<!-- @ -->
<!-- with these three quantities in hand, we get Figure~\ref{ts1} -->

<!-- \begin{figure}[htbp] -->
<!-- \begin{center} -->
<!-- <<tsone>>= -->
<!-- plot(time, foreigners/all.news, type='s') -->
<!-- @ -->
<!-- \caption{Attention to international affairs as a proportion of all news.  Calculated using the -->
<!-- true topic categories} -->
<!-- \label{ts1} -->
<!-- \end{center} -->
<!-- \end{figure} -->

<!-- Let's see how that \textit{would have} gone if we did not know the topic codes and had to use the classifier outputs.  -->

<!-- We recompute with the model's  -->
<!-- topic codes -->
<!-- <<crosstab3>>= -->
<!-- bigt.model <- with(testdata, table(Model.Topic.Code, month)) -->
<!-- foreigners.model <- colSums(bigt.model[c('19','16'),]) -->
<!-- @ -->
<!-- and replot, in Figure~\ref{ts2}.   -->

<!-- Finally we can plot the differences.  These are shown in Figure~\ref{diff} -->

<!-- \begin{figure}[htbp] -->
<!-- \begin{center} -->
<!-- <<tstwo>>= -->
<!-- plot(time,  foreigners.model/all.news, type='s') -->
<!-- @ -->
<!-- \caption{Attention to international affairs and defense as a proportion of all news.  Calculated using the -->
<!-- estimated topic categories} -->
<!-- \label{ts2} -->
<!-- \end{center} -->
<!-- \end{figure} -->

<!-- \begin{figure}[htbp] -->
<!-- \begin{center} -->
<!-- <<diff>>= -->
<!-- plot(foreigners/all.news, foreigners.model/all.news) -->
<!-- abline(a=0,b=1,col='red') -->
<!-- @ -->
<!-- \caption{Measurement error on a substantive question} -->
<!-- \label{diff} -->
<!-- \end{center} -->
<!-- \end{figure} -->



<!-- %% dta <- transform(data, Date=as.Date(Date, format='%d-%b-%y')) -->


<!-- %dd <- read.csv(nytimes.sample.csv) -->
<!-- %dd$quarter <- cut(dd$Date, 'quarter') -->
<!-- %dd$month <- cut(dd$Date, 'month') -->
<!-- %bigt <- with(dd, table(Topic, quarter)) -->
<!-- %plot(as.Date(colnames(bigt)), bigt['International Affairs',]/colSums(bigt), type='h') -->
<!-- %smallt <- with(dd, table(Topic, month)) -->
<!-- %plot(as.Date(colnames(bigt)), bigt['International Affairs',]/colSums(bigt), type='h') -->


<!-- %## now do this with the classified ones and check the diffs -->


<!-- %%%%%%%% now some topic modeling on the web -->


<!-- %  -->
<!-- %  -->
<!-- %%%%% now the old topic model code that we still don't want... -->
<!-- %  -->
<!-- % \subsection*{Topics in the UK Abortion Debate} -->
<!-- %  -->
<!-- % We dont have very large numbers of speeches in the UK abortion debate, but we can do  -->
<!-- % a topic analysis using paragraphs as documents.  A corpus of debate  -->
<!-- % paragraphs is included in the materials, called \texttt{debate-paras} for this -->
<!-- % purpose.  The first task is to use JFreq to get term document information in a suitable -->
<!-- % format. Drag all the paragraphs into JFreq and as a first run -->
<!-- % select the lowercase, no numbers, no currency, and English stemmer.  We've also included -->
<!-- % a list of stop words `stops.txt' that you can remove before processing.  This is just a csv file -->
<!-- % you can adjust if you feel the need. -->
<!-- %  -->
<!-- % Now choose an output folder name and make sure that the output format is `LDAC' and the gzip -->
<!-- % compression is not selected.  Other output formats, e.g. CSV are fine, but they will with  -->
<!-- % other corpora, make unnecessarily huge output files. -->
<!-- %  -->
<!-- % Shortly after you press the `Process' button, there will be a folder with the name you  -->
<!-- % chose wherever you decided to put it.  Let's have a quick look inside that folder. -->
<!-- %  -->
<!-- % You should see a file called README.txt.  This is an instruction\footnote{c.f. Alice at  -->
<!-- % the Mad Hatter's tea party.}.  It will explain the format of the files and how to use them. -->
<!-- %  -->
<!-- % Back at the R prompt, we can now make use of this information in the orginal topic models -->
<!-- % package.  First we load it. -->
<!-- % <<load-lda>>= -->
<!-- % library(lda) ## may need to install.packages('lda') first -->
<!-- % set.seed(1234) ## so results are replicable -->
<!-- % @ -->
<!-- % Now we extract the word count data and the list of words -->
<!-- % <<extract-info>>= -->
<!-- % counts <- read.documents("barapara-stemmed-stops/data.ldac") -->
<!-- % wds <- read.vocab("barapara-stemmed-stops/words.csv") -->
<!-- % @ -->
<!-- % and although the package itself does not require it, we pull out -->
<!-- % the document titles too -->
<!-- % <<paras>>= -->
<!-- % docs <- read.csv('barapara-stemmed-stops/docs.csv', header=FALSE, as.is=TRUE) -->
<!-- % @ -->
<!-- % (Note that this file doesn't have a header row.) -->
<!-- % Now fit an LDA model using some hopefully not unreasonable starting parameters -->
<!-- % <<fit-lda-model>>= -->
<!-- % mod <- lda.collapsed.gibbs.sampler(counts, 6, wds, 1000, alpha=0.01, eta=0.1) -->
<!-- % @ -->
<!-- % Looking at the help page for the collapsed gibbs sampler function you can confirm that -->
<!-- % we are fitting six topics, running for one thousand iterations with prior parameters -->
<!-- % $\alpha$=0.01 and $\eta$=0.1.  The closer $\alpha$ is to 0 the more each document  -->
<!-- % will tend to contain instances of fewer rather than more topics.   -->
<!-- % The closer $\eta$ is to 0 the more a topic will -->
<!-- % generate fewer words with high probability.  Altering $\alpha$ is often the primary  -->
<!-- % determinant of the distribution of topics that are inferred.  You can, of course, -->
<!-- % play around with this.  Alternatively, some topic model packages will allow you to -->
<!-- % estimate it too (but not this one unfortunately). -->
<!-- %  -->
<!-- % Now we want to have a look at the topics we inferred.  Here we list the words most likely -->
<!-- % to be generated on each topic.  We show the top fifteen -->
<!-- % <<top-topics,comment=''>>= -->
<!-- % top.topic.words(mod$topics, 15) -->
<!-- % @ -->
<!-- % If we have a particular interest in some estiamted topic we can also see which documents -->
<!-- % make most use of it.  Assume that we care about topic number 6 -->
<!-- % <<top-docs,comment=''>>= -->
<!-- % topdocs <- top.topic.documents(mod$document_sums, 15) -->
<!-- % docindexes <- topdocs[,6]  -->
<!-- % docs[docindexes,] ## index into the documents -->
<!-- % @ -->
<!-- % Notice that these are overwhelming paragraphs from the speakers that voted no  -->
<!-- % in the debate. -->
<!-- %  -->
<!-- % In both these cases we are manipulating elements of the fitted topic model. -->
<!-- % We could instead use them directly.   -->
<!-- %  -->
<!-- % If you want the topic assignments themselves then -->
<!-- % the \texttt{mod\$document\_sums} part of the model contains them with documents as rows -->
<!-- % and topic indexes as columns, pretty much how a Yoshikoder dictionary report would give them to you, -->
<!-- % only without the labels. -->

<!-- %\subsection*{Topics in the US Abortion Debate} -->
<!-- % -->
<!-- %If you're feeling adventurous you can try the same analysis on the US abortion debates,  -->
<!-- %again split by paragraph. -->

<!-- \end{document} -->


