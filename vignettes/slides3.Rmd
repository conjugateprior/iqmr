---
title: "Computer Assisted Content Analysis (3)"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: 
  rmarkdown::beamer_presentation:
    keep_tex: true
    latex_engine: xelatex
    template: rmarkdownbeamertemplate.tex
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Menu

Session 0: How could this possibly work?

Session 1: Dictionary-based 'classical' content analysis and topic models

Session 2: Classification and evaluation

Session 3: Scaling models
 
* Documents in space
* Modeling relative emphasis
* Validating human judgement
* Dimensionality
 

 

## Documents in (ideological) space

~\\
{\footnotesize
\begin{tabular}{rrrrrrrr} \toprule
          & neue & vor &Menschen& wie &nur & Arbeitsplätze & ... \\ \midrule
...\\
FDP-2005  &    11 & 20  &      6 & 22  &31 &            17 & ...\\
FDP-2002  &    17 & 17  &     27 & 30  &35  &            9 & ...\\
PDS-2005  &     5 & 10  &     17 & 10  & 9   &          12& ...\\
PDS-2002  &    15 & 19  &      8  & 9  & 3    &          9& ...\\
GREENS-2005  & 42 & 21    &   47 & 46 & 19 &            17& ...\\
GREENS-2002  & 27 & 18    &   27 & 28  &22 &            21& ...\\
SPD-2005  &     8 & 15 &      26 & 11 & 13     &        10& ...\\
SPD-2002  &    16 & 18 &      16 & 16 &  9      &        7& ...\\
CDU-2005  &    21 & 12 &      10 & 13 & 19       &      22& ...\\
CDU-2002  &    20 & 20 &      14 & 15 & 18        &      7& ...\\ 
...\\
\bottomrule
\end{tabular}
}

~\\
\centerline{{\small Manifestos as bags of words}}

 

## Documents in (ideological) space

e.g. the CMP \textcolor{gray}{(Budge et al. 1983)}.

~\\
\begin{center}
{\footnotesize
\begin{tabular}{rl} \toprule
Topic code & Meaning \\ \midrule 
403 & Market Regulation \\
404 & Economic Planning \\
405 & Corporatism \\ 
\vdots & \\
601 & National Way of Life: Positive \\
602 & National Way of Life: Negative \\
603 & Traditional Morality: Positive \\
604 & Traditional Morality: Negative \\
605 & Law and Order \\
 \bottomrule
\end{tabular}
}
\end{center}

 
## Documents in (ideological) space

e.g. the CMP \textcolor{gray}{(Budge et al. 1983)}.

~\\
\begin{center}
{\footnotesize
\begin{tabular}{rlrrrrrrrrrr}
  \toprule
  & 201 & 202 & 403 & 404 & 405 & 601 & ... \\ 
  \midrule
    ... \\
    FDP-1990 & 2 & 19 & 28 & 0 & 0 & 0 & \dots \\ 
    FDP-1994 & 0 & 11 & 17 & 0 & 0 & 0 &\dots \\ 
    FDP-1998 & 6 & 0 & 8 & 0 & 10 & 20 & \dots \\ 
    FDP-2002 & 26 & 11 & 31 & 1 & 0 & 10 & \dots \\ 
    FDP-2005 & 12 & 27 & 55 & 8 & 0 & 7 & \dots \\ 
    FDP-2009 & 10 & 38 & 16 & 21 & 0 & 10 & \dots \\ 
    ...\\
   \bottomrule
\end{tabular}
}
\end{center}

~\\
\centerline{{\small Manifestos as bags of topics}}
 

## Back to the (contingency) table

Recall our two assumptions:
 
* A matrix of document by word/topic counts is a *contingency table* generated by unobserved *positions*
* Words occur at a *rate* determined by the content they are being used to express
\begin{align*}
C_{ij} & \sim \text{Poisson}(\lambda_{ij})
\end{align*}
 

 

## Back to the (contingency) table

How to connect the rates of each word in a document to $\theta$s (and $\beta$s)

\begin{center}
{\tiny
\begin{tabular}{rrrrrrrrl} \toprule
          & neue & vor &Menschen& wie &nur & Arbeitsplätze & ... \\ \midrule
...\\
FDP-2005  &    11 & 20  &      6 & 22  &31 &            17 & ... 
& $\textcolor{bloodred}{\theta_\text{FDP-2005}}$\\
FDP-2002  &    17 & 17  &     27 & 30  &35  &            9 & ...
& $\textcolor{bloodred}{\theta_\text{FDP-2002}}$\\
PDS-2005  &     5 & 10  &     17 & 10  & 9   &          12& ...
& $\textcolor{bloodred}{\theta_\text{PDS-2005}}$\\
PDS-2002  &    15 & 19  &      8  & 9  & 3    &          9& ...
& $\textcolor{bloodred}{\theta_\text{PDS-2002}}$\\
GREENS-2005  & 42 & 21    &   47 & 46 & 19 &            17& ...
& $\textcolor{bloodred}{\theta_\text{GREENS-2005}}$\\
GREENS-2002  & 27 & 18    &   27 & 28  &22 &            21& ...
& $\textcolor{bloodred}{\theta_\text{GREENS-2002}}$\\
SPD-2005  &     8 & 15 &      26 & 11 & 13     &        10& ...
& $\textcolor{bloodred}{\theta_\text{SPD-2005}}$\\
SPD-2002  &    16 & 18 &      16 & 16 &  9      &        7& ...
& $\textcolor{bloodred}{\theta_\text{SPD-2002}}$\\
CDU-2005  &    21 & 12 &      10 & 13 & 19       &      22& ...
& $\textcolor{bloodred}{\theta_\text{CDU-2005}}$\\
CDU-2002  &    20 & 20 &      14 & 15 & 18        &      7& ... 
& $\textcolor{bloodred}{\theta_\text{CDU-2002}}$\\
...\\
          & $\textcolor{darkblue}{\beta_\text{neue}}$ & 
           $\textcolor{darkblue}{\beta_\text{vor}}$ & 
           $\textcolor{darkblue}{\beta_\text{Menschen}}$ & 
           $\textcolor{darkblue}{\beta_\text{wie}}$ & 
           $\textcolor{darkblue}{\beta_\text{nur}}$ & 
           $\textcolor{darkblue}{\beta_\text{Arbeitsplätze}}$ &  \\ 
\bottomrule
\end{tabular}
}
\end{center}

 


## Back to the (contingency) table

How to connect the rates of each word in a document to $\theta$s (and $\beta$s)

~\\
\centerline{\includegraphics[scale=0.4]{pictures/scaling.png}}

 

## Simple models of count data

There are two *log-linear models* of any contingency table
\begin{align*}
\text{log}\, \mu_{ij} & = \alpha_{i} + \psi_{j} & \text{({boring})}\\ 
              & = \alpha_i + \psi_{j} + \lambda_{ij} & \text{({pointless})}
\end{align*}

 
## Where's the relative emphasis?

Two models:
There are two *log-linear models* of any contingency table
\begin{align*}
\text{log}\, \mu_{ij} & = \alpha_{i} + \psi_{j} & \text{({independence})}\\ 
                      & = \alpha_i + \psi_{j} + \textcolor{bloodred}{\lambda_{ij}} & \text{({saturated})}
\end{align*}

~\\
All the *relative emphasis* 
(and all the *political position-taking*)
is in {\color{bloodred}$\lambda$}
 
* Scaling models give dimensional structure to {\color{bloodred}$\lambda$}.
 

 
## Infer dimensional structure

Intuition: $\lambda$ has an orthogonal decomposition
\begin{align*}
\lambda & = \Theta\Sigma B^T & \text{{\normalsize (SVD)}}\\
                  &= \sum^{M}_{m} \theta_{(m)} \sigma_{(m)} \beta_{(m)}^T\\
                  &\approx {\color{bloodred}\theta}\,{\color{darkgreen}\sigma}\,{\color{darkblue}\beta}^T & \text{{\normalsize (Rank 1 approx.)}}
\end{align*}. . .
{\color{bloodred}$\theta$} are *document positions*, {\color{darkblue}$\beta$} are *word positions*

 

## 
\centerline{\includegraphics[scale=0.5]{pictures/poissonscaling}}
 

## 
\centerline{\includegraphics[scale=0.4]{pictures/sp-informativeness}}
 

## Infer dimensional structure

Intuition: $\lambda$ has a orthogonal decomposition
\begin{align*}
\lambda & = \Theta\Sigma B^T & \text{{\normalsize (SVD)}}\\
                  &= \sum^{M}_{m} \theta_{(m)} \sigma_{(m)} \beta_{(m)}^T\\
                  &\approx \theta\,{\color{darkgreen}\sigma}\,\beta^T & \text{{\normalsize (Rank 1 approx.)}}
\end{align*}
{\color{darkgreen}$\sigma$} says *how much relative emphasizing* is happening in this dimension

. . . 
Right now, there's only one dimension so it's not so interesting... 

 
## Intuition

\vfill
\centerline{What are we doing when we fit such a model?}
\vfill

 
## A generative model of positioning text
\begin{center}
\includegraphics[scale=.5]{pictures/ip-schematic2}

\begin{align*}
\text{log}~\mu_{ij}~ & \textcolor{black}{= r_i + c_j ~+~}\frac{(p_i - b_j)^2}{v} 
\end{align*}
\end{center}


 
## This is just our model with a false moustache and  hat

Quadratic unfolding (Elff 2013, Heiser 1986) has the model as a reduced form

\begin{align*}
\text{log}~\mu_{ij}~ & \textcolor{black}{= r_i + c_j ~+~}\frac{(p_i - b_j)^2}{v}\\
             & \textcolor{gray}{= r_i + c_j + (p_i^2 - 2p_i b_j + b_j^2) / v}\\
             & \textcolor{gray}{= [r_i + p_i^2 / v] + [c_j + b_j^2 / v] + [p_i]\,[1/v] [-2 b_j]}\\
             & \textcolor{black}{= ~~~~~~\alpha_i \,~~~~~~~+ ~~~~~~~~\psi_j~~~ + ~~}
             \textcolor{bloodred}{\theta_i}~~~~\textcolor{darkgreen}{\sigma}\,~~~~~\textcolor{darkblue}{\beta_j}
\end{align*}

 
## Just like spatial voting

~\\
{\small
\centerline{\includegraphics[scale=.5]{pictures/ip-schematic3}}
}

~\\
*Two* words/topics, e.g. 'benefits' and 'assets', with scores $\beta_1$ and $\beta_2$ in a document of length $N_i$

 
## Just like spatial voting

{\small
\centerline{\includegraphics[scale=.5]{pictures/ideal}}
}

From \textcolor{pale}{Clinton et al. (2004)} 

 
## Just like spatial voting

{\small
\begin{align*} 
[C_{i1}, C_{i2}] &\sim \text{Binomial}( [{\pi}_{i1}, \pi_{i2}],  N_{i} )\\
\pi_{i1} &= \mu_{i1} / (\mu_{i1} + \mu_{i2}) \\
\text{log}\!\bigg(\frac{\pi_{i1}}{\pi_{i2}}\bigg) &= \text{log}~ \pi_{i1} - \text{log}~ \pi_{i2}\\
 &= (\alpha_i - \alpha_i) + (\psi_1 - \psi_2) + \textcolor{bloodred}{\theta_i}\,(\textcolor{darkblue}{\beta_1} - \textcolor{darkblue}{\beta_2})\\
 &= ~~~~~~~~~~~~~~~~~\;\;\;\;\;\,\psi_{1/2}~~\;\;\, + \theta_i\;~\;\;\;\beta_{1/2}
\end{align*}
}

. . .

Look Ma, a logit!

 

## Special case: logit scores

Identify left L and right R topic and compute (Lowe et al. 2011)
\begin{align*}
\hat{\theta}_i = \text{log}\,\,\, \frac{\sum_{j \in R}C_{ij}}{\sum_{k \in L}C_{ik}}
\end{align*}
. . .
Position is relative *proportional* emphasis, with a psychophysical motivation

 
## Revisiting human judgement

~\\

\centerline{\includegraphics[scale=.6]{pictures/smoky-room}}

~\\
\centerline{\textcolor{pale}{(Budge et al. 1983, Baumgartner and Jones)}}

 
## Validating what comes out of the smoky room

The CMP project have performed a huge manual content analysis and *chosen* some right and left topics for us.
 
* This kind of thing is a popular exercise (unless you're the coder)
 

We're supposed to add both sides up and subtract to get a position measure for documents

. . .

Are these topics really used by parties on right and left?
 
* Let's run our model on the topic output and check
* Turns out our $\hat{\theta}$s correlate 0.94 with their scale
* We're more interested in $\beta$s
 

 

## 
\centerline{\includegraphics[scale=.4]{pictures/grey-topics-just-rile}}

 
## 
\centerline{\includegraphics[scale=.4]{pictures/grey-topics-not-rile}}

 



## Dimension issues

\vfill
\centerline{What the heck is $\theta$?}

\centerline{How can we be sure that there is only one of them?}
\vfill

 
## What is $\theta$?

Whatever maximizes the Likelihood... 

. . .
Like all scaling techniques (e.g. NOMINATE), this model is *exploratory* -- *you* have to figure out what the dimension really is. 


 
## One dimensional world

How do we know that positions on only one dimension are being expressed? 

Relatedly: how do we get positions on a specific policy issue?

. . .

Three possibilities
 
* Use only those texts (or sections thereof) that are guaranteed to be on the same topic and *scale them separately* (Slapin and Proksch, 2008)
* Learn items from just a subset of relevant documents (Laver et al. 2003)
* Work with *topic* counts rather than word counts (Baerg and Lowe, MS)
 

Heroic assumptions are (closer to being) true



 
## Multidimensional world

Allow for more dimensions! $\theta^1_{i}$, $\theta^2_{i}$, ... 

We need to move to a computationally cheaper model: 
 
* Correspondence analysis (Greenacre 2007)
 

For identification, a K-dimensional model has K sets of $\theta$ and K sets of $\beta$ 
 
* and they'll be orthogonal...
 

. . .

Fit this model to the German topic counts...

 

## 
\centerline{\includegraphics[scale=.5]{pictures/grey-just-rile}}
 

## (Graduate student) life skills

How to read a biplot:
 
* Documents points are closer when using words/topics *similarly*
* Words points are closer with *similar* document profiles
* 0,0: a document or word/topic used *exactly as often as we would expect by chance*
* Document vector: arrow from 0,0 to a document point
* Word/topic vector: arrow from 0,0 to a word/topic point
* Vectors are *longer* the more their usage diverges from chance
* *Angle* between a word vector and document vector: how much a document preferentially uses the word 
 

 
## (Graduate student) life skills

There is nothing special to text about a biplot

This interpretation works for *all kinds* of cross-tables.

Use it for good!

 

## Dimensions and topic change

What if the political lexicon changes over time? (it does)
 
* New issues appear, old issues disappear
 

Then scaling algorithms pick up shifts in the policy agenda rather than shifts in party positions. 

 

## 
\centerline{\includegraphics[scale=.8]{pictures/de-2d}}
 

## Worst Case Scenario
\centerline{\includegraphics[scale=.6]{pictures/HandMfig2}}
 

## Lab Time

\centerline{\includegraphics[scale=.8]{pictures/hieroglyphic-keyboard-cm}}

 
