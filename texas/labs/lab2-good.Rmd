---
title: "Lab 2"
author: "Will Lowe"
date: "11 January 2015"
output: html_document
---

## Text Classification: What's in the New York Times?

In this demonstration we will be using a corpus of article titles and description from
the New York Times.  The classification task is to determine which of 27 subject categories
a story is from.  You can find a list of these categories in the accompanying document.

We begin by loading the library we will be using for classification 
```{r loading,results='hide',warning=FALSE,message=FALSE}
library(RTextTools) ## may need to install.packages('RTextTools') first
```
The documents we will be using are in the file on the course pages
```{r make-data}
load("nytimes-sample.RData") ## object is called nytimes.sample
```
This is a *data.matrix*, so we first see what sorts of variables we have available to us
```{r variables}
colnames(nytimes.sample)
```
We will be most interested in the text fields `Subject` and `Title` and the numerical subject category label `Topic.Code`, which is conveniently translated as `Topic`.  For convenience the code to topic mapping is shown in the table.

```{r code-table}
codes <- read.csv('topics-nyt.csv', row.names=1)
```

```{r code-table2,results='asis',echo=FALSE}
library(xtable)
print(xtable(codes, caption='New York Times Index topic codes and their descriptive names', label='codes'), type='html')
```
Let's take a look at the first five 'documents'
(which are essentially NY Times headlines). Which topic would you assign to these?
```{r preview}
head(nytimes.sample[,c('Title', 'Subject')])
```
You can see whether your coding corresponds to the actual labels human coders assigned them
```{r preview2}
head(nytimes.sample[,c('Topic','Topic.Code')])
```
In order to build a document classifier we first 
transform this nice readable dataset into a sparse matrix of word counts.

As part of the process of doing that we'll take the opportunity to throw out number words, reduce 
to word stems, and weight the resulting word counts using a standard term 
frequency-inverse document frequency (tfidf) transformation.  
```{r make-matrix}
matrix <- create_matrix(cbind(nytimes.sample$Title,nytimes.sample$Subject),
                        language="english",
                        removeNumbers=TRUE, 
                        stemWords=TRUE, 
                        weighting=tm::weightTfIdf)
```
where the `cbind` function ties together two columns from `nytimes.sample`.
We handed in the `Title` and `Subject` fields together
because we expect that they both provide information about the topic code.  We
could have done more (or less) pre-processing as we counted and transformed the
words.  See the help for `create_matrix` for details.

Now that we have a suitable representation for our documents we create a container object
to hold both them and their category labels.  We'll use the container to train and
also to test fitted classifiers
```{r make-corpus}
set.seed(1234) ## so your cases will be mine
training.cases <- 1:2000
test.cases <- 2001:3104
corpus <- create_container(matrix, nytimes.sample$Topic.Code, 
                           trainSize=training.cases, 
                           testSize=test.cases, 
                           virgin=FALSE)
```
 Here we specify the matrix we just made out of the documents as the first argument,
 the column of category labels as the second, and then a division into training
 and test documents.  The not-quite-accurately named `trainSize` parameter
 contains the row numbers that we want to use to fit the classifiers, here the 
 first thousand documents.  The similarly inaccurately names `testSize` 
 parameters indicates that we want to test them on the remaining documents.  The
 bizarrely named `virgin` parameter is false, indicating that we actually
 do know the correct labels for the test documents.  This will not, of 
 course, always be true.
 
 The library is set up to run lots of classifiers and have them vote on each test 
 document.  But for now let's just train one, regularised 
 logistic regression which computational linguists (for reasons best known
 to themselves) refer to as the maximum entropy classifier or 'maxent'.
 
```{r train}
 models <- train_models(corpus, algorithms=c("SVM"))
``` 
Normally in R we would want to summarise a fitted model.  In this case however, the 
 parameters are unlikely to be particularly illuminating (and there are several
 hundred of them), so no such methods are provided.
 
 If we had wanted to train more or different models, we would have had to put their 
 names in the `algorithms` parameter.  The options are 
```{r algorithms}
print_algorithms()
```
 and you can spend a happy afternoon reading about them on the internet.  (Mac users might want to avoid 'maxent' in this version)
 
Now that our single classifier is trained we would like to see how well it works
```{r classify}
results <- classify_models(corpus, models)
```
 This particular function hides a fair amount of detail about the model, which is 
 usually what we want.  Looking at the first few results 
```{r results}
head(results)
```
 we see the topic in the first column and the probability that the classifier
 assigned this topic in the second column.  Despite more than 20 possible categories this model seems rather confident.  This is not necessarily a good thing.  
 
 As a first look at the 
 performance of the model, let us see what how likely it is
 to put a document in the right category, and then examine what sorts of errors are made.
 
 The library comes with various forms of 'analytics', 
 that is: performance summaries.  Let's 
 compute all of them for the corpus we trained with and the results we got
```{r analytics}
analytics <- create_analytics(corpus, results) 
```
 The analytics object is a vast sprawling thing that you can read about on its help page.
 
 One summary we can make from it is the probability that the classifier puts a 
 document in the right category (recall), which we might want to break down by category.  Another is the
 probability that a document is in a category when the model says it is (precision).
 Actually we want these things often enough that they get their own function, the idiosyncratically punctuated 
```{r precision-recall}
 pr <- create_precisionRecallSummary(corpus, results)
```
 which we show in the Table, including the F-measure which combines precision and recall.  The is useful if you like reducing vast amounts of disparate information to single number summaries.  We don't, so we'll ignore it.
% 
 
```{r pr-table,echo=FALSE,results='asis'}
colnames(pr) <- c('precision', 'recall', 'F') ## work around underscore bug in memisc's toLatex 
print(xtable(pr, digits=2, caption="Precision, recall and the F measure for classifier performance", label="pr"), type='html')
```
 
What difference do these errors make?  One way to check this we can make a time series of Defense/International Affairs stories as a proportion of all New York Times stories and compare it to the time series we would have gotten if we had used our classifier's decisions instead.

First, let's pull out the rows corresponding to test documents from the original
data set
```{r truecats}
testdata <- nytimes.sample[test.cases,]
```
and add the model's predictions about their topic as a new column
```{r modelests}
testdata$Model.Topic.Code <- results[,1] ## just the topic code 
```
To make our time series we have to aggregate a bit, so let's add a monthly indicator variable.
```{r make-cut}
testdata$month <- cut(testdata$Date, 'month')
```
This will allow us to aggregate documents that appear within the same month. This smooths the 
data a bit.
```{r crosstab1}
bigt <- with(testdata, table(Topic.Code, month))
```
This big cross tabulation has months as columns and aggregated topic code counts as rows.  It's
a bit unwieldy to look at, so let us summarise the amount of attention given to 'International Affairs'
and `Defense' (codes 19 and 16) as a proportion of all the different sorts of news.
```{r crosstab2}
all.news <- colSums(bigt)
foreigners <- colSums(bigt[c('19','16'),])  ## careful with the quotes
time <- as.Date(colnames(bigt)) ## turn our indicator back into date
```
with these three quantities in hand, we get a Figure.

```{r tsone,fig.cap="Attention to international affairs as a proportion of all news.  Calculated using the true topic categories"}
plot(time, foreigners/all.news, type='s')
```

Let's see how that *would have* gone if we did not know the topic codes and had to use the classifier outputs. 

We recompute with the model's 
topic codes
```{r crosstab3}
bigt.model <- with(testdata, table(Model.Topic.Code, month))
foreigners.model <- colSums(bigt.model[c('19','16'),])
```
and replot.

Finally we can plot the differences.
```{r tstwo,fig.cap="Attention to international affairs and defense as a proportion of all news.  Calculated using the estimated topic categories"}
plot(time,  foreigners.model/all.news, type='s')
```

```{r tsthree,fig.cap="Measurement error on a substantive question"}
plot(foreigners/all.news, foreigners.model/all.news, xlim=c(0,1), ylim=c(0,1))
abline(a=0,b=1,col='red')
```

