---
title: "Lab 3"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_width: 7
    fig_height: 7 
vignette: >
  %\VignetteIndexEntry{Lab 3}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## US Senate Speeches

Let's take a look at a US Senate debate on partial birth abortion.
As always, we'll load the texts, make a corpus, then a document term matrix to start off
```{r}
library(quanteda)
library(readtext)
library(dplyr)

library(iqmr)
```

<!-- the data is in a folder called "abortion-debate-us-senate" in the 'usual place': -->
<!-- ```{r} -->
<!-- # where this data lives on your machine -->
<!-- DATA_DIR <- file.path(system.file(package = "readtext"), "vignette/data") -->
<!-- folder <- file.path(DATA_DIR, "abortion-debate-us-senate") -->
<!-- ``` -->
<!-- Then we make a document feature matrix to fit a model to -->

<!-- Let's take quick look at it these documents: -->
<!-- ```{r} -->
<!-- dir(folder) -->
<!-- ``` -->

<!-- We'll make a corpus and use their rather convenient naming scheme to create docvars. -->
<!-- ```{r} -->
<!-- txts <- readtext(folder, docvarsfrom = "filenames", docvarnames = c("Party", "Speaker")) -->
<!-- corp <- corpus(txts) -->
<!-- summary(corp) -->
<!-- ``` -->

<!-- ## Different version -->

We'll use the pre-constructed data frame of speaker contributions and make a
corpus out of it
```{r}
data("df_pba_debate_by_speaker")
names(df_pba_debate_by_speaker)

corp <- corpus(df_pba_debate_by_speaker, text_field = "contributions", docid_field = "speaker")
docvars(corp, "speaker") <- df_pba_debate_by_speaker$speaker # add a speaker field
```


Now we'll make a document term matrix (what quanteda calls a `dfm`)
```{r}
corpdfm <- dfm(corp)
```
Before we took care to remove a lot of things we though would be confusing.  
Here let's live dangerously and use all the words.  (The models being fit are
a lot more constrained than before).  You should check these decisions don't
affect much.

We'll run a simple one dimensional scaling model, known to 'wordfish' to political
scientists, and an association model to everone else
```{r}
wfish <- textmodel_wordfish(corpdfm, dir = c(3, 21))
summary(wfish)
```
the estimated position can be extracted as if this were a regular fitted model
```{r}
preds  <- predict(wfish, interval = "confidence")
preds
```

Annoyingly we need to work a bit to extract the (only) element in `preds` 
(called 'fit') to get a table of positions and associated uncertainty, and
add the speaker information
```{r}
# make a data frame from fit matrix and the document variables
pos <- data.frame(docvars(corpdfm), preds$fit) %>%
  arrange(fit)
pos <- pos[order(pos$fit),] # sort 
```
Let's plot these positions.  I'll use `ggplot2`
```{r}
library(ggplot2)
library(ggrepel) # for labelling
theme_set(theme_minimal())

ggplot(pos, aes(x = fit, y = 1:nrow(pos), 
                xmin = lwr, xmax = upr, 
                col = party)) +
  geom_point() +
  geom_errorbarh(height = 0) +
  scale_color_manual(values = c("blue", "red")) +
  scale_y_continuous(labels = pos$speaker, breaks = 1:nrow(pos)) +
  labs(x = "Position", y = "Speaker") +
  ggtitle("Estimated Positions from Senate Partial-Birth Abortion Debate",
          subtitle = "October 21st, 2003")
```

We can also get one of those nice `Eiffel Tower' plots that Proksch and Slapin use.

Let's extract the word scores for later:
```{r}
wscores <- data.frame(word = wfish$features,
                      score = wfish$beta,
                      offset = wfish$psi)
```

We can get some idea of the nature of the debate language by looking for
words we think ought to be charged.  Take a moment to make some predictions
about the scores of these:
```{r}
testwords <- c("life", "choice", "womb", "her", "woman", "health",
               "born", "baby", "little", "gruesome", "kill", "roe",
               "wade", "medical", "her", "his", "child", "religion",
               "catholic", "doctor", "nurse")
```

Now let's see
```{r}
testscores <- wscores %>%
  filter(word %in% testwords) %>%
  arrange(score)

testscores[,1:2] # just word and score columns
```

If we were Slapin and Proksch we might put all this on one of their Eiffel Tower
diagrams...

```{r}
ggplot(wscores, aes(score, offset, label = word)) +
  geom_point(color = "grey", alpha = 0.2) +
  geom_text_repel(data = testscores, col = "black") +
  geom_point(data = testscores) +
  labs(x = "Word score", y = "Offset parameter") +
  ggtitle("Estimated Word Positions for Selected Debate Vocabulary",
          subtitle = "Note: Offset parameter is roughly proportional to word frequency")
```


If we were being thorough about these words we'd check they do what we think they do by looking by looking at them in all their contexts, as we did before. Try it.

```{r, eval = FALSE}
kwic(corp, "baby", window = 15)
```


<!-- We can also look at more than one dimension in this data.  For this we'll use the \texttt{ca} package.  You may need use to \texttt{install.package} this first. -->
<!-- <<>>= -->
<!-- library(ca) -->
<!-- dim(senatewfm) ## we need to flip this around for ca -->
<!-- mod2 <- ca(t(senatewfm), nf=2) ## note transpose t -->
<!-- @ -->
<!-- The \texttt{ca} package calls its $\theta$s \texttt{rowcoord} and $\beta$ \texttt{colcoord}. -->

<!-- Although this is a least squares approximation to the wordfish model, the approximation is pretty good.  Let's compare the first dimension with wordfish's document positions.  We'll correlate because the (arbitrary) scaling is different between models -->
<!-- <<>>= -->
<!-- catheta <- mod2$rowcoord[,1] -->
<!-- cor(catheta, mod$theta) -->
<!-- @ -->
<!-- Basically the same, and it's much quicker to fit too\ldots -->

<!-- The \texttt{summary} method is pretty comprehensive, though you'll probably want to read some of Greenacre 2007 to mwds <- mod$wordsake the most of it. -->
<!-- <<eval=FALSE>>= -->
<!-- summary(mod2) -->
<!-- @ -->
<!-- Since we've got multiple dimensions we can check how much variation is being explained in each.  What the slides called $\sigma$ is related to the singular values of the underlying SVD, which we can get from the model. Let's plot these -->
<!-- <<>>= -->
<!-- plot(mod2$sv, pch=16) -->
<!-- @ -->
<!-- The `elbow' after the first dimension is one (fallible) reason to think that this debate is mostly one dimensional. That is at least theoretically plausible. -->

<!-- If you want to see a biplot of all the words and documents, then -->
<!-- <<eval=FALSE>>= -->
<!-- plot(mod2) -->
<!-- @ -->
<!-- but be warned.  It's big\ldots  You may want to read the help page to see how to only show some elements, or to change the colors. -->

<!-- \end{document} -->
