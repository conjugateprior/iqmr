---
title: "Day 1"
author: "Will Lowe"
date: "12 January 2015"
output: html_document
---

## Using the R package mallet

Let's fit a topic model to the Bara et al. abortion debate data.  Here we set a root folder, relative to which the materials can be found.  You should be able to change this and recompile to run the code on your own machine. 
```{r}
ROOT <- '/Users/will/Dropbox/teaching/texas/'
library(mallet)
```
We'll specify the data folder relative to this root and read in the documents there.  These documents are actually paragraphs from the debate, numbered and labelled with the speaker, in order to give us enough data to fit models
```{r}
folder <- file.path(ROOT, 'abortion-debate-by-para')
dd <- mallet.read.dir(folder)
```
We'll remove the stop words before model fitting.  The R package requires we provide a list of stopwords in a file.  Here it's called 'stops.txt'.  Then we import the document data.
```{r}
stopsfile <- file.path(ROOT, 'labs', 'stops.txt')
mallet.instances <- mallet.import(dd$id, dd$text, stopsfile,  
      token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
```
Importing here means, deciding where the words are in each file and removing stop words.  For this example we'll use a regular expression to define what counts as a word.  This one says: 'at least one letter, zero or more letters and punctuation marks, and a letter to end'.  

Finally, once we've got a representation of the words in each document we define (but don't yet fit) a topic model.  Here we choose 20 topics, mostly just to see the broad thematic outline of this corpus. 
```{r}
topic.model <- MalletLDA(num.topics=20)
```
We add the instance list we created earlier
```{r}
topic.model$loadDocuments(mallet.instances)
```
ask to optimise the document hyper-parameters once ever 10 iterations of the Gibbs sampler
```{r}
topic.model$setAlphaOptimization(10, 200)
```
and run 2000 iterations.  
```{r}
topic.model$train(2000)
```
If you do this at the R prompt you'll see the topics and log likelihoods in each iteration scrolling by.  From the R package we don't see this, but we can ask the trained object for the relevant information.  

Notice also that our syntax is quite un-R-like.  We train the model and set its parameters in place.  That's because in the background we're really just calling the Java code.  We can do that more directly from the commandline as shown below, or even more directly by linking to the Mallet libraries.

Let's now take a look at the words associated with each topic
```{r}
topic.words <- mallet.topic.words(topic.model, 
      smoothed=TRUE, normalized=TRUE) 
```
We've asked for the counts to be normalised so that the sum of word parameters in each topic sums to one.  This represents the probabilities that we generate each word when the unobserved topic marker Z indicates that topic.  Otherwise the numbers are the number of word tokens that are thought to have been generated by that topic.  Normalisation means we include the effects of the prior. 
```{r}
doc.topics <- mallet.doc.topics(topic.model, 
      smoothed=T, normalized=T)
```
We can see the words most highly associated with, e.g. topic 8 by
```{r}
mallet.top.words(topic.model, topic.words[8,])
```
though we might need a few more to figure out what this topic actually means
```{r}
mallet.top.words(topic.model, topic.words[8,], 20) ## twenty
```
Finally we can take a look at a few documents that are estimated to have large amounts of topic 8:
```{r}
head(dd[ doc.topics[8,] > 0.05, ])
```
and we can generate some labels for the topics using the highest probability words
```{r}
topic.labels <- mallet.topic.labels(topic.model, topic.words)
```
This function has a bug in it, preventing you from seeing any more than 3 words, but we can locally override the function to see more. Just paste the following at your prompt 
```{r}
mallet.topic.labels <- function(topic.model, 
            topic.words, num.top.words = 3){
    n.topics <- dim(topic.words)[1]
    topics.labels <- rep("", n.topics)
    for (topic in 1:n.topics) 
      topics.labels[topic] <- paste(mallet.top.words(topic.model, 
          topic.words[topic, ], num.top.words)$words, collapse = " ")
    topics.labels
}
```
and put the number of words you want as the third argument when you call the function.

We can plot the results of the model balancing docuemnt level and word level similarity
```{r}
clust <- mallet.topic.hclust(doc.topics, topic.words, 0.3)
plot(clust, labels=topic.labels)
```
Finally we can get the document topic assignment matrix
```{r}
out <- mallet.doc.topics(topic.model, 
                         normalized=FALSE, smoothed=FALSE)
```
Here we've asked for the raw counts of topic assignments, so each row should sum to the number of words in it (minus any stop words we might have removed).  This is the same form of output we woud have gotten from a dictionary-based content analysis -- except that we have learnt the dictionary from the data. 

## Using mallet from the commandline

Assuming you have set up your path to be able to call the `mallet` command directly we can do many of the same things -- and a few more besides -- by calling the same code from the commandline.  For Mac and Linus users this is a Terminal prompt.  For Windows users this is the rather horrible black thing you get when you type CMD on the start menu.

Mallet's commandline interface is fairly lightly documented, but you can get help on all the possible commands by typing
```{r eval=FALSE}
mallet --help
```
and more specific help by appending the help flag to a particular command.  For example, to get a list of options for `train-topics` type
```{r eval=FALSE}
mallet train-topics --help
```

Here's a simple sequence of commands to do what we did before in R.  Actually we can fit a slightly wider range of models from the commandline than from R, which is one reason to use it.  To fit the full range of models in Mallet (and there are a lot) then we'd need to write some Java code, but that's beyond the scope of this course.

We'll start by loading in the same data as before.  There's a default stopword list built in so we'll use that rather than passing one in as we did before
```{r eval=FALSE}
mallet import-dir 
      --input /Users/will/Dropbox/teaching/texas/abortion-debate-by-para 
      --remove-stopwords --keep-sequence
```
the output of this command is by default called `text.vectors`, though we can name the file explicitly if we want (see `mallet import-dir --help` for the relevant option name).

Then we train up a topic model
```{r eval=FALSE}
mallet train-topics --input text.vectors 
      --output-topic-keys topickeys --optimize-interval 10 
      --random-seed 1234 --num-iterations 2000
```
Here we've requested the top topic words to be dropped into a file called `topickeys`, 2000 iterations of the Gibbs sampler, and a hyperparameter update every 10 iterations.  If we like this model we need to add
```{r eval=FALSE}
--output-doc-topics
```
to get the model's view of the topics in this set of document.

We might instead be interested in the N-gram variant of LDA that extracts word sequences for each topic too.  To use this we have to keep more information when we process the initial documents, like this
```{r eval=FALSE}
mallet import-dir --input /Users/will/Dropbox/teaching/texas/abortion-debate-by-para --remove-stopwords --keep-sequence-bigrams TRUE
```
now we can train the topic model using ngram information.
```{r eval=FALSE}
mallet train-topics --input text.vectors --output-topic-keys topickeys --optimize-interval 10 --use-ngrams true --random-seed 1234 --num-iterations 2000
```
This is just a sketch of how to use this interface, you can do a lot more with it.  Also, Mallet is written by people actively developing topic models -- notable David Mimno and Andrew McCallum -- so you are likely to get more up to date code and algorithms.

## Alternatives

There are a *lot* of topic model toolkits out there, so you should find the one you get on with best.  

For R there are *three* alternatives if you don't like `mallet`.  The `lda` package uses Bei's code and only fits simple LDA models.  The `topicmodels` package does the same with its own codebase, and the `stm` package fits structural topic models.  `stm` provides a wide range of visualisation tools, but it's in heavy development, so it will have (and every now and again gain) bugs, and you'll need to do a bit more work to get documents into it.  

## Data formats

Large scale text analysis requires large amounts of text, which as we have seen, is always sparse.  Consequently spreadsheet formatted word counts won't scale (too much space is wasted storing zeros) and a more suitable format is required.  One useful format widely used in computer text analysis is LDA-C, after the package of the same name. It's not very human-readable since it only stores no-zero word counts and identifies words by their indices, but it's very compact. Many packages either require it or can read it.  

You can create it with a wide variety of software, including `JFreq`, the `ykwords` commandline package or the python script `text2ldac.py`  and many others.


