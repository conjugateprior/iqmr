\documentclass[11pt,compress,professionalfonts]{beamer}
\usetheme{metropolis}

\usepackage[]{graphicx}
\usepackage[]{color}

\usepackage{ctable}
\usepackage{hyperref}
\usepackage{dcolumn}
\usepackage{booktabs}

\usepackage{mathspec}
\setsansfont{Avenir Next}[BoldFont={* Demi Bold}]
\setmonofont[Scale = MatchLowercase,Mapping=tex-ansi]{Inconsolata}
\setmathfont(Digits,Latin,Greek){Avenir Next}
\newfontface\fancy{Avenir Light}[Mapping=tex-ansi]

\definecolor{darkgray}{rgb}{0.5,0.5,0.5}
\definecolor{darkblue}{rgb}{0.000,0.251,0.502}
\definecolor{bloodred}{rgb}{0.502,0.000,0.000}
\definecolor{palered}{rgb}{0.7,0.000,0.000}
\definecolor{darkgreen}{rgb}{0.000,0.502,0.0}
\definecolor{verylightgray}{rgb}{0.9,0.9,0.9}
\definecolor{lightgray}{rgb}{0.75,0.75,0.75}
\definecolor{pale}{rgb}{0.75,0.75,0.75}

\setbeamerfont{frametitle}{size=\Large,family=\fancy}
\setbeamercolor{frametitle}{fg=bloodred, bg=verylightgray}
\setbeamerfont{title}{family=\fancy,size=\Large}
\setbeamercolor{title}{fg=bloodred}
\setbeamercolor{background canvas}{bg=white}
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{title separator}{fg=darkgray}

\metroset{progressbar=none,numbering=none}

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

%% bullet free bullet points
\newcommand{\ita}{\begin{itemize}}
\newcommand{\itm}{\item[]}
\newcommand{\itz}{\end{itemize}}
\setbeamercolor{itemize item}{fg=lightgray}

%\usepackage{bayesnet} %% needs to be installed: ~/Library/texmf/tex/latex/bayesnet

% graphics file in the pics subdirectory please
%\graphicspath{{./pictures/}}

\title{Computer-assisted content analysis}
\author{Will Lowe ~~\textcolor{gray}{Princeton University}\\
        James Lo ~~ \textcolor{gray}{University of Southern California}}
\date{}
\begin{document}

\maketitle



\begin{frame}[t]\frametitle{Menu}

Session 0: How could this possibly work?

Session 1: Dictionary-based `classical' content analysis and topic models

Session 2: Classification and evaluation

Session 3: Scaling models
\ita
\itm Documents in space
\itm Modeling relative emphasis
\itm Validating human judgement
\itm Dimensionality
\itz

\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]\frametitle{Documents in (ideological) space}

~\\
{\footnotesize
\begin{tabular}{rrrrrrrr} \toprule
          & neue & vor &Menschen& wie &nur & Arbeitsplätze & \ldots \\ \midrule
...\\
FDP-2005  &    11 & 20  &      6 & 22  &31 &            17 & \ldots\\
FDP-2002  &    17 & 17  &     27 & 30  &35  &            9 & \ldots\\
PDS-2005  &     5 & 10  &     17 & 10  & 9   &          12& \ldots\\
PDS-2002  &    15 & 19  &      8  & 9  & 3    &          9& \ldots\\
GREENS-2005  & 42 & 21    &   47 & 46 & 19 &            17& \ldots\\
GREENS-2002  & 27 & 18    &   27 & 28  &22 &            21& \ldots\\
SPD-2005  &     8 & 15 &      26 & 11 & 13     &        10& \ldots\\
SPD-2002  &    16 & 18 &      16 & 16 &  9      &        7& \ldots\\
CDU-2005  &    21 & 12 &      10 & 13 & 19       &      22& \ldots\\
CDU-2002  &    20 & 20 &      14 & 15 & 18        &      7& \ldots\\ 
...\\
\bottomrule
\end{tabular}
}

~\\
\centerline{{\small Manifestos as bags of words}}

\end{frame}

\begin{frame}[t]\frametitle{Documents in (ideological) space}

e.g. the CMP \textcolor{gray}{(Budge et al. 1983)}.

~\\
\begin{center}
{\footnotesize
\begin{tabular}{rl} \toprule
Topic code & Meaning \\ \midrule 
403 & Market Regulation \\
404 & Economic Planning \\
405 & Corporatism \\ 
\vdots & \\
601 & National Way of Life: Positive \\
602 & National Way of Life: Negative \\
603 & Traditional Morality: Positive \\
604 & Traditional Morality: Negative \\
605 & Law and Order \\
 \bottomrule
\end{tabular}
}
\end{center}

\end{frame}
\begin{frame}[t]\frametitle{Documents in (ideological) space}

e.g. the CMP \textcolor{gray}{(Budge et al. 1983)}.

~\\
\begin{center}
{\footnotesize
\begin{tabular}{rlrrrrrrrrrr}
  \toprule
  & 201 & 202 & 403 & 404 & 405 & 601 & \ldots \\ 
  \midrule
    ... \\
    FDP-1990 & 2 & 19 & 28 & 0 & 0 & 0 & \dots \\ 
    FDP-1994 & 0 & 11 & 17 & 0 & 0 & 0 &\dots \\ 
    FDP-1998 & 6 & 0 & 8 & 0 & 10 & 20 & \dots \\ 
    FDP-2002 & 26 & 11 & 31 & 1 & 0 & 10 & \dots \\ 
    FDP-2005 & 12 & 27 & 55 & 8 & 0 & 7 & \dots \\ 
    FDP-2009 & 10 & 38 & 16 & 21 & 0 & 10 & \dots \\ 
    ...\\
   \bottomrule
\end{tabular}
}
\end{center}

~\\
\centerline{{\small Manifestos as bags of topics}}
\end{frame}

\begin{frame}[t]\frametitle{Back to the (contingency) table}

Recall our two assumptions:
\ita
\item A matrix of document by word/topic counts is a \textit{contingency table} generated by unobserved \textit{positions}
\item Words occur at a \textit{rate} determined by the content they are being used to express
\begin{align*}
C_{ij} & \sim \text{Poisson}(\lambda_{ij})
\end{align*}
\itz

\end{frame}

\begin{frame}[t]\frametitle{Back to the (contingency) table}

How to connect the rates of each word in a document to $\theta$s (and $\beta$s)

\begin{center}
{\tiny
\begin{tabular}{rrrrrrrrl} \toprule
          & neue & vor &Menschen& wie &nur & Arbeitsplätze & \ldots \\ \midrule
...\\
FDP-2005  &    11 & 20  &      6 & 22  &31 &            17 & \ldots 
& $\textcolor{bloodred}{\theta_\text{FDP-2005}}$\\
FDP-2002  &    17 & 17  &     27 & 30  &35  &            9 & \ldots
& $\textcolor{bloodred}{\theta_\text{FDP-2002}}$\\
PDS-2005  &     5 & 10  &     17 & 10  & 9   &          12& \ldots
& $\textcolor{bloodred}{\theta_\text{PDS-2005}}$\\
PDS-2002  &    15 & 19  &      8  & 9  & 3    &          9& \ldots
& $\textcolor{bloodred}{\theta_\text{PDS-2002}}$\\
GREENS-2005  & 42 & 21    &   47 & 46 & 19 &            17& \ldots
& $\textcolor{bloodred}{\theta_\text{GREENS-2005}}$\\
GREENS-2002  & 27 & 18    &   27 & 28  &22 &            21& \ldots
& $\textcolor{bloodred}{\theta_\text{GREENS-2002}}$\\
SPD-2005  &     8 & 15 &      26 & 11 & 13     &        10& \ldots
& $\textcolor{bloodred}{\theta_\text{SPD-2005}}$\\
SPD-2002  &    16 & 18 &      16 & 16 &  9      &        7& \ldots
& $\textcolor{bloodred}{\theta_\text{SPD-2002}}$\\
CDU-2005  &    21 & 12 &      10 & 13 & 19       &      22& \ldots
& $\textcolor{bloodred}{\theta_\text{CDU-2005}}$\\
CDU-2002  &    20 & 20 &      14 & 15 & 18        &      7& \ldots 
& $\textcolor{bloodred}{\theta_\text{CDU-2002}}$\\
...\\
          & $\textcolor{darkblue}{\beta_\text{neue}}$ & 
           $\textcolor{darkblue}{\beta_\text{vor}}$ & 
           $\textcolor{darkblue}{\beta_\text{Menschen}}$ & 
           $\textcolor{darkblue}{\beta_\text{wie}}$ & 
           $\textcolor{darkblue}{\beta_\text{nur}}$ & 
           $\textcolor{darkblue}{\beta_\text{Arbeitsplätze}}$ &  \\ 
\bottomrule
\end{tabular}
}
\end{center}


\end{frame}
\begin{frame}[t]\frametitle{Simple models of count data}

There are two \textit{log-linear models} of any contingency table
\begin{align*}
\text{log}\, \mu_{ij} & = \alpha_{i} + \psi_{j} & \text{({boring})}\\ 
              & = \alpha_i + \psi_{j} + \lambda_{ij} & \text{({pointless})}
\end{align*}

\end{frame}
\begin{frame}[t]\frametitle{Where's the relative emphasis?}

Two models:
There are two \textit{log-linear models} of any contingency table
\begin{align*}
\text{log}\, \mu_{ij} & = \alpha_{i} + \psi_{j} & \text{({independence})}\\ 
                      & = \alpha_i + \psi_{j} + \textcolor{bloodred}{\lambda_{ij}} & \text{({saturated})}
\end{align*}

~\\
All the \textit{relative emphasis} 
(and all the \textit{political position-taking})
is in {\color{bloodred}$\lambda$}
\ita
\itm Scaling models give dimensional structure to {\color{bloodred}$\lambda$}.
\itz

\end{frame}
\begin{frame}[t]\frametitle{Infer dimensional structure}

Intuition: $\lambda$ has an orthogonal decomposition
\begin{align*}
\lambda & = \Theta\Sigma B^T & \text{{\normalsize (SVD)}}\\
                  &= \sum^{M}_{m} \theta_{(m)} \sigma_{(m)} \beta_{(m)}^T\\
                  &\approx {\color{bloodred}\theta}\,{\color{darkgreen}\sigma}\,{\color{darkblue}\beta}^T & \text{{\normalsize (Rank 1 approx.)}}
\end{align*}\pause
{\color{bloodred}$\theta$} are \textit{document positions}, {\color{darkblue}$\beta$} are \textit{word positions}

\end{frame}

\begin{frame}[plain]
\centerline{\includegraphics[scale=0.5]{pictures/poissonscaling}}
\end{frame}

\begin{frame}[plain]
\centerline{\includegraphics[scale=0.4]{pictures/sp-informativeness}}
\end{frame}

\begin{frame}[t]\frametitle{Infer dimensional structure}

Intuition: $\lambda$ has a orthogonal decomposition
\begin{align*}
\lambda & = \Theta\Sigma B^T & \text{{\normalsize (SVD)}}\\
                  &= \sum^{M}_{m} \theta_{(m)} \sigma_{(m)} \beta_{(m)}^T\\
                  &\approx \theta\,{\color{darkgreen}\sigma}\,\beta^T & \text{{\normalsize (Rank 1 approx.)}}
\end{align*}
{\color{darkgreen}$\sigma$} says \textit{how much relative emphasizing} is happening in this dimension

\pause 
Right now, there's only one dimension so it's not so interesting\ldots 

\end{frame}
\begin{frame}[t]\frametitle{Intuition}

\vfill
\centerline{What are we doing when we fit such a model?}
\vfill

\end{frame}
\begin{frame}[t]\frametitle{A generative model of positioning text}
\begin{center}
\includegraphics[scale=.5]{pictures/ip-schematic2}

\begin{align*}
\text{log}~\mu_{ij}~ & \textcolor{black}{= r_i + c_j ~+~}\frac{(p_i - b_j)^2}{v} 
\end{align*}
\end{center}


\end{frame}
\begin{frame}[t]\frametitle{This is just our model with a false moustache and  hat}

Quadratic unfolding (Elff 2013, Heiser 1986) has the model as a reduced form

\begin{align*}
\text{log}~\mu_{ij}~ & \textcolor{black}{= r_i + c_j ~+~}\frac{(p_i - b_j)^2}{v}\\
             & \textcolor{gray}{= r_i + c_j + (p_i^2 - 2p_i b_j + b_j^2) / v}\\
             & \textcolor{gray}{= [r_i + p_i^2 / v] + [c_j + b_j^2 / v] + [p_i]\,[1/v] [-2 b_j]}\\
             & \textcolor{black}{= ~~~~~~\alpha_i \,~~~~~~~+ ~~~~~~~~\psi_j~~~ + ~~}
             \textcolor{bloodred}{\theta_i}~~~~\textcolor{darkgreen}{\sigma}\,~~~~~\textcolor{darkblue}{\beta_j}
\end{align*}

\end{frame}
\begin{frame}[t]\frametitle{Just like spatial voting}

~\\
{\small
\centerline{\includegraphics[scale=.5]{pictures/ip-schematic3}}
}

~\\
\textit{Two} words/topics, e.g. 'benefits' and 'assets', with scores $\beta_1$ and $\beta_2$ in a document of length $N_i$

\end{frame}
\begin{frame}[t]\frametitle{Just like spatial voting}

{\small
\centerline{\includegraphics[scale=.5]{pictures/ideal}}
}

From \textcolor{pale}{Clinton et al. (2004)} 

\end{frame}
\begin{frame}[t]\frametitle{Just like spatial voting}

{\small
\begin{align*} 
[C_{i1}, C_{i2}] &\sim \text{Binomial}( [{\pi}_{i1}, \pi_{i2}],  N_{i} )\\
\pi_{i1} &= \mu_{i1} / (\mu_{i1} + \mu_{i2}) \\
\text{log}\!\bigg(\frac{\pi_{i1}}{\pi_{i2}}\bigg) &= \text{log}~ \pi_{i1} - \text{log}~ \pi_{i2}\\
 &= (\alpha_i - \alpha_i) + (\psi_1 - \psi_2) + \textcolor{bloodred}{\theta_i}\,(\textcolor{darkblue}{\beta_1} - \textcolor{darkblue}{\beta_2})\\
 &= ~~~~~~~~~~~~~~~~~\;\;\;\;\;\,\psi_{1/2}~~\;\;\, + \theta_i\;~\;\;\;\beta_{1/2}
\end{align*}
}

\pause

Look Ma, a logit!

%Quadratic utility generates a decision boundary (log-)linear in the 
%difference between word/topic positions.
\end{frame}

\begin{frame}[t]\frametitle{Special case: logit scores}

Identify left L and right R topic and compute (Lowe et al. 2011)
\begin{align*}
\hat{\theta}_i = \text{log}\,\,\, \frac{\sum_{j \in R}C_{ij}}{\sum_{k \in L}C_{ik}}
\end{align*}
\pause
Position is relative \textit{proportional} emphasis, with a psychophysical motivation

\end{frame}
\begin{frame}[t]\frametitle{Revisiting human judgement}

~\\

\centerline{\includegraphics[scale=.6]{pictures/smoky-room}}

~\\
\centerline{\textcolor{pale}{(Budge et al. 1983, Baumgartner and Jones)}}

\end{frame}
\begin{frame}[t]\frametitle{Validating what comes out of the smoky room}

The CMP project have performed a huge manual content analysis and \textit{chosen} some right and left topics for us.
\ita
\itm This kind of thing is a popular exercise (unless you're the coder)
\itz

We're supposed to add both sides up and subtract to get a position measure for documents

\pause

Are these topics really used by parties on right and left?
\ita
\itm Let's run our model on the topic output and check
\itm Turns out our $\hat{\theta}$s correlate 0.94 with their scale
\itm We're more interested in $\beta$s
\itz

\end{frame}

\begin{frame}[plain]
\centerline{\includegraphics[scale=.4]{pictures/grey-topics-just-rile}}

\end{frame}
\begin{frame}[plain]
\centerline{\includegraphics[scale=.4]{pictures/grey-topics-not-rile}}

\end{frame}


% \begin{frame}[plain]
% \end{frame}
% 
% \begin{frame}[t]\frametitle{}
% Typically scaling models assume that
% \ita
% \itm relative word usage is reflective of political ideology
% \itm Positions are unidimensional in $W$ 
% \itm Positions drive word counts stochastically according to a particular form for $P(W_{j} \mid \theta)$
% \itm Bag of words: counts of $W_{j}$ are conditionally independent given $\theta$
% \[
% P(W_{1}\ldots W_{V}) ~=~ \prod^{V}_{j} P(W_{j} \mid \theta)~ P(\theta)
% \] 
% \itz 
% 
% \end{frame}
% \begin{frame}[t]\frametitle{Existing Models}
% 
% We focus on:
% \ita
% \itm \textit{Wordfish}: (Slapin and Proksch, 2008)
% \itm (Also Laver et al. 2003, Monroe and Maeda, 2004; Beauchamp, 2008; Pennings and Keman, 2002; K\"{o}nig and Luig, 2009, Goodman 1979, etc.)
% \itz
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \end{frame}
% \begin{frame}[t]\frametitle{Wordfish}
% 
% The position word relationship is
% \begin{eqnarray*}
% W_{ij} &\sim& \text{Poisson}(\mu_{ij})\\
% \log \mu_{ij} &=& \psi_{j} + \beta_{j}\theta_{i} +  \alpha_{i} 
% \end{eqnarray*}
% Each word is a Poisson Process (stochastic component) driven by word and document parameters (the systematic component)
% 
% 
% Word parameters:
% \ita
% \itm $\beta$~~ how fast counts increase or decrease with changes in position
% \itm $\psi$~~ how frequent words are irrespective of position
% \itz
% 
% 
% \end{frame}
% \begin{frame}[t]\frametitle{Wordfish}
% 
% The position word relationship is
% \begin{eqnarray*}
% W_{ij} &\sim& \text{Poisson}(\mu_{ij})\\
% \log \mu_{ij} &=& \psi_{j} + \beta_{j}\theta_{i} +  \alpha_{i}
% \end{eqnarray*}
% Each word is a Poisson Process driven by word and document parameters
% 
% Document parameters:
% \ita
% \itm $\theta$~~ the position being expressed
% \itm $\alpha$~~ a fixed effects for documents controlling for length\ldots
% \itz
% 
% \end{frame}

\begin{frame}[t]\frametitle{Dimension issues}

\vfill
\centerline{What the heck is $\theta$?}

\centerline{How can we be sure that there is only one of them?}
\vfill

\end{frame}
\begin{frame}[t]\frametitle{What is $\theta$?}

Whatever maximizes the Likelihood\ldots 

\pause
Like all scaling techniques (e.g. NOMINATE), this model is \textit{exploratory} -- \textit{you} have to figure out what the dimension really is. 


\end{frame}
\begin{frame}[t]\frametitle{One dimensional world}

How do we know that positions on only one dimension are being expressed? 

Relatedly: how do we get positions on a specific policy issue?

\pause

Three possibilities
\ita
\item Use only those texts (or sections thereof) that are guaranteed to be on the same topic and \textit{scale them separately} (Slapin and Proksch, 2008)
\item Learn items from just a subset of relevant documents (Laver et al. 2003)
\item Work with \textit{topic} counts rather than word counts (Baerg and Lowe, MS)
\itz

Heroic assumptions are (closer to being) true



\end{frame}
\begin{frame}[t]\frametitle{Multidimensional world}

Allow for more dimensions! $\theta^1_{i}$, $\theta^2_{i}$, \ldots 

We need to move to a computationally cheaper model: 
\ita
\itm Correspondence analysis (Greenacre 2007)
\itz

For identification, a K-dimensional model has K sets of $\theta$ and K sets of $\beta$ 
\ita
\itm and they'll be orthogonal\ldots
\itz

\pause

Fit this model to the German topic counts\ldots

\end{frame}

\begin{frame}[plain]
\centerline{\includegraphics[scale=.5]{pictures/grey-just-rile}}
\end{frame}

\begin{frame}[t]\frametitle{(Graduate student) life skills}

How to read a biplot:
\ita
\item Documents points are closer when using words/topics \textit{similarly}
\item Words points are closer with \textit{similar} document profiles
\item 0,0: a document or word/topic used \textit{exactly as often as we would expect by chance}
\item Document vector: arrow from 0,0 to a document point
\item Word/topic vector: arrow from 0,0 to a word/topic point
\item Vectors are \textit{longer} the more their usage diverges from chance
\item \textit{Angle} between a word vector and document vector: how much a document preferentially uses the word 
\itz

\end{frame}
\begin{frame}\frametitle{(Graduate student) life skills}

There is nothing special to text about a biplot

This interpretation works for \textit{all kinds} of cross-tables.

Use it for good!

\end{frame}

\begin{frame}[t]\frametitle{Dimensions and topic change}

What if the political lexicon changes over time? (it does)
\ita
\itm New issues appear, old issues disappear
\itz

Then scaling algorithms pick up shifts in the policy agenda rather than shifts in party positions. 

\end{frame}

\begin{frame}[plain]
\centerline{\includegraphics[scale=.8]{pictures/de-2d}}
\end{frame}

\begin{frame}[t]\frametitle{Worst Case Scenario}
\centerline{\includegraphics[scale=.6]{pictures/HandMfig2}}
\end{frame}

\begin{frame}[t]\frametitle{Lab Time}

\centerline{\includegraphics[scale=.8]{pictures/hieroglyphic-keyboard-cm}}

\end{frame}


\end{document}
